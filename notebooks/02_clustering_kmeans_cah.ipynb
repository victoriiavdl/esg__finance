{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2 -- Clustering K-means et Classification Ascendante Hierarchique (CAH)\n",
    "\n",
    "## Objectif\n",
    "\n",
    "Ce notebook applique les deux methodes de clustering utilisees dans le paper de reference \n",
    "aux donnees ESG de notre panel d'entreprises technologiques :\n",
    "\n",
    "1. **K-means** : partitionnement en k groupes minimisant l'inertie intra-cluster\n",
    "2. **CAH (clustering hierarchique agglomeratif)** : construction bottom-up d'une hierarchie de clusters\n",
    "\n",
    "On applique ces methodes sur deux jeux de variables :\n",
    "- Les **niveaux absolus** des scores (Environmental, Social, ESG global)\n",
    "- Les **variations annuelles** des scores (delta sur 12 mois)\n",
    "\n",
    "## Pre-requis\n",
    "\n",
    "Executer le Notebook 1 au prealable pour generer les donnees nettoyees dans `data/clean/`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Imports et chargement des donnees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from scipy.spatial.distance import pdist\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples, calinski_harabasz_score, davies_bouldin_score\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (14, 7)\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['grid.alpha'] = 0.3\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "TICKER_NAMES = {\n",
    "    'NVDA US Equity': 'NVIDIA', 'AVGO US Equity': 'Broadcom',\n",
    "    'TSM US Equity': 'TSMC', 'GOOGL US Equity': 'Alphabet',\n",
    "    '000660 KS Equity': 'SK Hynix', 'LRCX US Equity': 'Lam Research',\n",
    "    '6857 JP Equity': 'Advantest', 'TSEM IT Equity': 'Tower Semi',\n",
    "    'MSFT US Equity': 'Microsoft', 'LITE US Equity': 'Lumentum',\n",
    "    'AMD US Equity': 'AMD', 'FN US Equity': 'Fabrinet',\n",
    "    'SNOW US Equity': 'Snowflake', 'MU US Equity': 'Micron',\n",
    "    'TSLA US Equity': 'Tesla', '9984 JP Equity': 'SoftBank',\n",
    "    'CRDO US Equity': 'CREDO Tech', 'ENR GR Equity': 'Siemens Energy',\n",
    "    'MPWR US Equity': 'Monolithic Power', '2383 TT Equity': 'Elite Material',\n",
    "    'CLS CN Equity': 'Celestica', 'META US Equity': 'Meta',\n",
    "    'AMZN US Equity': 'Amazon'\n",
    "}\n",
    "\n",
    "print('Imports effectues.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des donnees nettoyees\n",
    "df_env = pd.read_csv('../data/clean/env_score_clean.csv', parse_dates=['Date'])\n",
    "df_soc = pd.read_csv('../data/clean/soc_score_clean.csv', parse_dates=['Date'])\n",
    "df_esg = pd.read_csv('../data/clean/esg_score_clean.csv', parse_dates=['Date'])\n",
    "\n",
    "df_env_var = pd.read_csv('../data/clean/env_score_variations.csv', parse_dates=['Date'])\n",
    "df_soc_var = pd.read_csv('../data/clean/soc_score_variations.csv', parse_dates=['Date'])\n",
    "df_esg_var = pd.read_csv('../data/clean/esg_score_variations.csv', parse_dates=['Date'])\n",
    "\n",
    "df_panel = pd.read_csv('../data/clean/panel_complet.csv', parse_dates=['Date'])\n",
    "\n",
    "print(f\"Panel charge : {df_panel.shape[0]} observations, {df_panel['Ticker'].nunique()} entreprises\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Preparation des donnees pour le clustering\n",
    "\n",
    "### Choix de la representation\n",
    "\n",
    "Pour appliquer le clustering, il faut construire une matrice **entreprises x variables**. \n",
    "Plusieurs approches sont possibles :\n",
    "\n",
    "1. **Coupe transversale** : utiliser les scores a une date donnee (ex: derniere observation)\n",
    "2. **Moyennes temporelles** : moyenner les scores sur toute la periode\n",
    "3. **Multi-periodes** : utiliser les scores de plusieurs periodes comme variables\n",
    "\n",
    "Nous appliquons les trois approches. L'approche par moyennes est la plus robuste au bruit \n",
    "ponctuel et sera notre approche principale.\n",
    "\n",
    "### Standardisation\n",
    "\n",
    "La standardisation (centrage-reduction) est indispensable avant K-means car cet algorithme \n",
    "utilise la distance euclidienne, qui est sensible aux echelles. Meme si les trois scores BESG \n",
    "sont sur la meme echelle [0, 10], leurs distributions peuvent differer significativement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparer_matrice_clustering(df_env, df_soc, df_esg, methode='moyenne'):\n",
    "    \"\"\"\n",
    "    Prepare la matrice entreprises x variables pour le clustering.\n",
    "    \n",
    "    Parametres\n",
    "    ----------\n",
    "    df_env, df_soc, df_esg : pd.DataFrame\n",
    "        DataFrames en format wide (Date + colonnes de scores).\n",
    "    methode : str\n",
    "        'moyenne' : moyenne sur toute la periode\n",
    "        'derniere' : derniere observation disponible\n",
    "        'mediane' : mediane sur toute la periode\n",
    "    \n",
    "    Retourne\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Matrice entreprises x variables (Env, Soc, ESG).\n",
    "    \"\"\"\n",
    "    score_cols = [c for c in df_env.columns if c != 'Date']\n",
    "    \n",
    "    if methode == 'moyenne':\n",
    "        env_vals = df_env[score_cols].mean()\n",
    "        soc_vals = df_soc[score_cols].mean()\n",
    "        esg_vals = df_esg[score_cols].mean()\n",
    "    elif methode == 'derniere':\n",
    "        env_vals = df_env[score_cols].iloc[-1]\n",
    "        soc_vals = df_soc[score_cols].iloc[-1]\n",
    "        esg_vals = df_esg[score_cols].iloc[-1]\n",
    "    elif methode == 'mediane':\n",
    "        env_vals = df_env[score_cols].median()\n",
    "        soc_vals = df_soc[score_cols].median()\n",
    "        esg_vals = df_esg[score_cols].median()\n",
    "    else:\n",
    "        raise ValueError(f\"Methode inconnue : {methode}\")\n",
    "    \n",
    "    mat = pd.DataFrame({\n",
    "        'Env_Score': env_vals,\n",
    "        'Soc_Score': soc_vals,\n",
    "        'ESG_Score': esg_vals\n",
    "    })\n",
    "    \n",
    "    mat.index.name = 'Ticker'\n",
    "    mat['Entreprise'] = mat.index.map(TICKER_NAMES)\n",
    "    \n",
    "    # Supprimer les entreprises avec trop de NA (moins de 50% d'observations)\n",
    "    mat = mat.dropna(thresh=3)  # Au moins les 3 scores disponibles\n",
    "    \n",
    "    return mat\n",
    "\n",
    "\n",
    "# Preparation avec la methode des moyennes\n",
    "mat_moyenne = preparer_matrice_clustering(df_env, df_soc, df_esg, methode='moyenne')\n",
    "mat_derniere = preparer_matrice_clustering(df_env, df_soc, df_esg, methode='derniere')\n",
    "\n",
    "print(f\"Matrice de clustering (moyennes) : {mat_moyenne.shape[0]} entreprises x 3 variables\")\n",
    "print(f\"Matrice de clustering (derniere obs) : {mat_derniere.shape[0]} entreprises x 3 variables\")\n",
    "print(\"\\nApercu (moyennes) :\")\n",
    "mat_moyenne.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardisation\n",
    "features = ['Env_Score', 'Soc_Score', 'ESG_Score']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(mat_moyenne[features])\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=features, index=mat_moyenne.index)\n",
    "\n",
    "print(\"Donnees standardisees (moyenne=0, ecart-type=1) :\")\n",
    "print(f\"  Moyennes apres standardisation : {X_scaled.mean(axis=0).round(6)}\")\n",
    "print(f\"  Ecarts-types apres standardisation : {X_scaled.std(axis=0).round(6)}\")\n",
    "print(\"\\nApercu :\")\n",
    "X_scaled_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. K-means : determination du nombre optimal de clusters\n",
    "\n",
    "Le choix du nombre de clusters k est un probleme central en analyse de clustering. \n",
    "Nous utilisons trois criteres complementaires :\n",
    "\n",
    "1. **Methode du coude (Elbow)** : on trace l'inertie intra-cluster (WCSS) en fonction de k et on cherche le \"coude\" ou la decroissance ralentit\n",
    "2. **Score de silhouette** : mesure de la coherence des clusters, entre -1 (mauvais) et +1 (excellent)\n",
    "3. **Indice de Calinski-Harabasz** : ratio variance inter/intra-cluster, a maximiser\n",
    "4. **Indice de Davies-Bouldin** : mesure de separation entre clusters, a minimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des metriques pour k = 2 a 10\n",
    "K_range = range(2, min(11, len(X_scaled)))  # Pas plus de clusters que d'observations\n",
    "\n",
    "inertias = []\n",
    "silhouettes = []\n",
    "calinski = []\n",
    "davies_bouldin = []\n",
    "\n",
    "for k in K_range:\n",
    "    km = KMeans(n_clusters=k, n_init=50, max_iter=500, random_state=42)\n",
    "    labels = km.fit_predict(X_scaled)\n",
    "    \n",
    "    inertias.append(km.inertia_)\n",
    "    silhouettes.append(silhouette_score(X_scaled, labels))\n",
    "    calinski.append(calinski_harabasz_score(X_scaled, labels))\n",
    "    davies_bouldin.append(davies_bouldin_score(X_scaled, labels))\n",
    "\n",
    "print(\"Metriques calculees pour k = 2 a\", max(K_range))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des 4 criteres\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Elbow\n",
    "axes[0, 0].plot(list(K_range), inertias, 'bo-', linewidth=2, markersize=8)\n",
    "axes[0, 0].set_xlabel('Nombre de clusters (k)')\n",
    "axes[0, 0].set_ylabel('Inertie intra-cluster (WCSS)')\n",
    "axes[0, 0].set_title('Methode du coude (Elbow)')\n",
    "axes[0, 0].set_xticks(list(K_range))\n",
    "\n",
    "# Silhouette\n",
    "axes[0, 1].plot(list(K_range), silhouettes, 'go-', linewidth=2, markersize=8)\n",
    "best_k_sil = list(K_range)[np.argmax(silhouettes)]\n",
    "axes[0, 1].axvline(x=best_k_sil, color='red', linestyle='--', \n",
    "                    label=f'Optimal : k={best_k_sil}')\n",
    "axes[0, 1].set_xlabel('Nombre de clusters (k)')\n",
    "axes[0, 1].set_ylabel('Score de silhouette')\n",
    "axes[0, 1].set_title('Score de silhouette moyen')\n",
    "axes[0, 1].set_xticks(list(K_range))\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Calinski-Harabasz\n",
    "axes[1, 0].plot(list(K_range), calinski, 'ro-', linewidth=2, markersize=8)\n",
    "best_k_ch = list(K_range)[np.argmax(calinski)]\n",
    "axes[1, 0].axvline(x=best_k_ch, color='blue', linestyle='--', \n",
    "                    label=f'Optimal : k={best_k_ch}')\n",
    "axes[1, 0].set_xlabel('Nombre de clusters (k)')\n",
    "axes[1, 0].set_ylabel('Indice de Calinski-Harabasz')\n",
    "axes[1, 0].set_title('Indice de Calinski-Harabasz (a maximiser)')\n",
    "axes[1, 0].set_xticks(list(K_range))\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Davies-Bouldin\n",
    "axes[1, 1].plot(list(K_range), davies_bouldin, 'mo-', linewidth=2, markersize=8)\n",
    "best_k_db = list(K_range)[np.argmin(davies_bouldin)]\n",
    "axes[1, 1].axvline(x=best_k_db, color='red', linestyle='--', \n",
    "                    label=f'Optimal : k={best_k_db}')\n",
    "axes[1, 1].set_xlabel('Nombre de clusters (k)')\n",
    "axes[1, 1].set_ylabel('Indice de Davies-Bouldin')\n",
    "axes[1, 1].set_title('Indice de Davies-Bouldin (a minimiser)')\n",
    "axes[1, 1].set_xticks(list(K_range))\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.suptitle('Determination du nombre optimal de clusters -- K-means', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../notebooks/figures/02_optimal_k.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nResume des k optimaux :\")\n",
    "print(f\"  Silhouette      : k = {best_k_sil} (score = {max(silhouettes):.3f})\")\n",
    "print(f\"  Calinski-Harabasz: k = {best_k_ch} (score = {max(calinski):.1f})\")\n",
    "print(f\"  Davies-Bouldin  : k = {best_k_db} (score = {min(davies_bouldin):.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagramme de silhouette detaille pour le k optimal\n",
    "from matplotlib.cm import get_cmap\n",
    "\n",
    "k_optimal = best_k_sil  # On retient le k maximisant la silhouette\n",
    "\n",
    "km_final = KMeans(n_clusters=k_optimal, n_init=50, max_iter=500, random_state=42)\n",
    "labels_km = km_final.fit_predict(X_scaled)\n",
    "\n",
    "# Calcul des silhouettes individuelles\n",
    "sil_values = silhouette_samples(X_scaled, labels_km)\n",
    "sil_avg = silhouette_score(X_scaled, labels_km)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "y_lower = 10\n",
    "cmap = get_cmap('Set2')\n",
    "\n",
    "for i in range(k_optimal):\n",
    "    cluster_sil = sil_values[labels_km == i]\n",
    "    cluster_sil.sort()\n",
    "    y_upper = y_lower + len(cluster_sil)\n",
    "    \n",
    "    color = cmap(i / k_optimal)\n",
    "    ax.fill_betweenx(np.arange(y_lower, y_upper), 0, cluster_sil,\n",
    "                     facecolor=color, edgecolor=color, alpha=0.7)\n",
    "    ax.text(-0.05, y_lower + 0.5 * len(cluster_sil), f'Cluster {i+1}', fontsize=10)\n",
    "    y_lower = y_upper + 10\n",
    "\n",
    "ax.axvline(x=sil_avg, color='red', linestyle='--', linewidth=2,\n",
    "           label=f'Silhouette moyenne = {sil_avg:.3f}')\n",
    "ax.set_xlabel('Coefficient de silhouette')\n",
    "ax.set_ylabel('Entreprises')\n",
    "ax.set_title(f'Diagramme de silhouette -- K-means (k={k_optimal})')\n",
    "ax.legend(loc='lower right')\n",
    "ax.set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../notebooks/figures/02_silhouette_diagram.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Application du K-means avec le k optimal\n",
    "\n",
    "On applique K-means avec le nombre de clusters determine precedemment et on examine \n",
    "la composition de chaque cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajout des labels au DataFrame\n",
    "mat_moyenne['Cluster_KM'] = labels_km + 1  # Clusters 1, 2, ..., k\n",
    "\n",
    "# Composition des clusters\n",
    "print(f\"=== Resultats K-means (k = {k_optimal}) ===\\n\")\n",
    "\n",
    "for c in sorted(mat_moyenne['Cluster_KM'].unique()):\n",
    "    members = mat_moyenne[mat_moyenne['Cluster_KM'] == c]\n",
    "    print(f\"--- Cluster {c} ({len(members)} entreprises) ---\")\n",
    "    for _, row in members.iterrows():\n",
    "        print(f\"  {row['Entreprise']:20s}  Env={row['Env_Score']:.2f}  Soc={row['Soc_Score']:.2f}  ESG={row['ESG_Score']:.2f}\")\n",
    "    print(f\"  [Moyennes] Env={members['Env_Score'].mean():.2f}  Soc={members['Soc_Score'].mean():.2f}  ESG={members['ESG_Score'].mean():.2f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profil moyen de chaque cluster (radar chart / bar chart)\n",
    "cluster_profiles = mat_moyenne.groupby('Cluster_KM')[features].mean()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Bar chart des profils\n",
    "cluster_profiles.plot(kind='bar', ax=axes[0], colormap='Set2', edgecolor='black')\n",
    "axes[0].set_xlabel('Cluster')\n",
    "axes[0].set_ylabel('Score moyen')\n",
    "axes[0].set_title('Profil moyen de chaque cluster K-means')\n",
    "axes[0].set_xticklabels([f'Cluster {i}' for i in cluster_profiles.index], rotation=0)\n",
    "axes[0].legend(['Environmental', 'Social', 'ESG Global'])\n",
    "\n",
    "# Taille des clusters\n",
    "sizes = mat_moyenne['Cluster_KM'].value_counts().sort_index()\n",
    "axes[1].bar(sizes.index, sizes.values, color=[get_cmap('Set2')(i/k_optimal) for i in range(k_optimal)],\n",
    "            edgecolor='black')\n",
    "axes[1].set_xlabel('Cluster')\n",
    "axes[1].set_ylabel('Nombre d\\'entreprises')\n",
    "axes[1].set_title('Taille des clusters K-means')\n",
    "axes[1].set_xticks(sizes.index)\n",
    "\n",
    "for i, v in enumerate(sizes.values):\n",
    "    axes[1].text(sizes.index[i], v + 0.2, str(v), ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../notebooks/figures/02_kmeans_profiles.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation 2D des clusters (projection sur les deux premieres composantes principales)\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca_2d = PCA(n_components=2)\n",
    "X_pca = pca_2d.fit_transform(X_scaled)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "cmap = get_cmap('Set2')\n",
    "\n",
    "for c in sorted(mat_moyenne['Cluster_KM'].unique()):\n",
    "    mask = mat_moyenne['Cluster_KM'].values == c\n",
    "    ax.scatter(X_pca[mask, 0], X_pca[mask, 1], \n",
    "               c=[cmap((c-1)/k_optimal)], label=f'Cluster {c}', s=120,\n",
    "               edgecolors='black', linewidth=1, alpha=0.8)\n",
    "\n",
    "# Annotation des points\n",
    "entreprises = mat_moyenne['Entreprise'].values\n",
    "for i, name in enumerate(entreprises):\n",
    "    ax.annotate(name, (X_pca[i, 0], X_pca[i, 1]),\n",
    "                textcoords='offset points', xytext=(5, 5), fontsize=8)\n",
    "\n",
    "# Centroides projetes\n",
    "centroids_pca = pca_2d.transform(km_final.cluster_centers_)\n",
    "ax.scatter(centroids_pca[:, 0], centroids_pca[:, 1], c='red', marker='X', \n",
    "           s=300, edgecolors='black', linewidth=2, label='Centroides', zorder=5)\n",
    "\n",
    "ax.set_xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]*100:.1f}% de variance)')\n",
    "ax.set_ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]*100:.1f}% de variance)')\n",
    "ax.set_title(f'K-means (k={k_optimal}) -- Projection ACP 2D')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../notebooks/figures/02_kmeans_pca2d.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Variance expliquee par les 2 composantes : {pca_2d.explained_variance_ratio_.sum()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Classification Ascendante Hierarchique (CAH)\n",
    "\n",
    "La CAH est une methode de clustering hierarchique qui construit une hierarchie de clusters \n",
    "par fusions successives. Contrairement au K-means, elle ne necessite pas de specifier k a priori \n",
    "et produit un dendrogramme qui permet de visualiser la structure hierarchique des donnees.\n",
    "\n",
    "### Choix du critere de lien (linkage)\n",
    "\n",
    "Nous testons quatre criteres de lien :\n",
    "- **Ward** : minimise l'augmentation de l'inertie intra-cluster (le plus utilise)\n",
    "- **Complete** : distance maximale entre paires de points de deux clusters\n",
    "- **Average** : distance moyenne entre paires de points\n",
    "- **Single** : distance minimale (sensible au phenomene de chaining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des 4 types de linkage\n",
    "linkage_methods = ['ward', 'complete', 'average', 'single']\n",
    "linkage_results = {}\n",
    "\n",
    "for method in linkage_methods:\n",
    "    Z = linkage(X_scaled, method=method, metric='euclidean')\n",
    "    linkage_results[method] = Z\n",
    "\n",
    "print(\"Matrices de lien calculees pour :\", linkage_methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dendrogrammes comparatifs\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 14))\n",
    "\n",
    "short_labels = [TICKER_NAMES.get(t, t)[:12] for t in mat_moyenne.index]\n",
    "\n",
    "for ax, method in zip(axes.flat, linkage_methods):\n",
    "    dendrogram(linkage_results[method], labels=short_labels,\n",
    "               leaf_rotation=90, leaf_font_size=8, ax=ax,\n",
    "               color_threshold=0)\n",
    "    ax.set_title(f'Dendrogramme -- Lien {method.capitalize()}')\n",
    "    ax.set_ylabel('Distance')\n",
    "\n",
    "plt.suptitle('Comparaison des criteres de lien pour la CAH', fontsize=14, y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../notebooks/figures/02_dendrogrammes_comparaison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dendrogramme detaille avec le critere de Ward (reference)\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "Z_ward = linkage_results['ward']\n",
    "\n",
    "dend = dendrogram(Z_ward, labels=short_labels, leaf_rotation=45, \n",
    "                  leaf_font_size=10, ax=ax, above_threshold_color='gray')\n",
    "\n",
    "# Ligne de coupure pour k clusters\n",
    "# On determine le seuil de coupure pour avoir k_optimal clusters\n",
    "# La hauteur de coupure est entre les (n-k)-ieme et (n-k+1)-ieme fusions\n",
    "n = len(X_scaled)\n",
    "if k_optimal < n:\n",
    "    threshold = (Z_ward[-(k_optimal), 2] + Z_ward[-(k_optimal-1), 2]) / 2\n",
    "    ax.axhline(y=threshold, color='red', linestyle='--', linewidth=2,\n",
    "               label=f'Coupure pour k={k_optimal} clusters')\n",
    "\n",
    "ax.set_ylabel('Distance (Ward)')\n",
    "ax.set_title('Dendrogramme -- Critere de Ward (distance euclidienne)')\n",
    "ax.legend(fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../notebooks/figures/02_dendrogramme_ward.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application de la CAH avec le critere de Ward\n",
    "labels_cah = fcluster(Z_ward, t=k_optimal, criterion='maxclust')\n",
    "\n",
    "mat_moyenne['Cluster_CAH'] = labels_cah\n",
    "\n",
    "print(f\"=== Resultats CAH Ward (k = {k_optimal}) ===\\n\")\n",
    "\n",
    "for c in sorted(mat_moyenne['Cluster_CAH'].unique()):\n",
    "    members = mat_moyenne[mat_moyenne['Cluster_CAH'] == c]\n",
    "    print(f\"--- Cluster {c} ({len(members)} entreprises) ---\")\n",
    "    for _, row in members.iterrows():\n",
    "        print(f\"  {row['Entreprise']:20s}  Env={row['Env_Score']:.2f}  Soc={row['Soc_Score']:.2f}  ESG={row['ESG_Score']:.2f}\")\n",
    "    print(f\"  [Moyennes] Env={members['Env_Score'].mean():.2f}  Soc={members['Soc_Score'].mean():.2f}  ESG={members['ESG_Score'].mean():.2f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score de silhouette pour la CAH\n",
    "sil_cah = silhouette_score(X_scaled, labels_cah)\n",
    "ch_cah = calinski_harabasz_score(X_scaled, labels_cah)\n",
    "db_cah = davies_bouldin_score(X_scaled, labels_cah)\n",
    "\n",
    "print(f\"Metriques de qualite (CAH Ward, k={k_optimal}) :\")\n",
    "print(f\"  Silhouette        : {sil_cah:.3f}\")\n",
    "print(f\"  Calinski-Harabasz : {ch_cah:.1f}\")\n",
    "print(f\"  Davies-Bouldin    : {db_cah:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Comparaison K-means vs CAH\n",
    "\n",
    "On compare les deux methodes en termes de :\n",
    "- Concordance des affectations (quelles entreprises changent de cluster ?)\n",
    "- Metriques de qualite\n",
    "- Stabilite des resultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tableau de contingence K-means vs CAH\n",
    "contingence = pd.crosstab(\n",
    "    mat_moyenne['Cluster_KM'].rename('K-means'),\n",
    "    mat_moyenne['Cluster_CAH'].rename('CAH'),\n",
    "    margins=True\n",
    ")\n",
    "print(\"Tableau de contingence K-means vs CAH :\")\n",
    "print(contingence.to_string())\n",
    "\n",
    "# Indice de Rand ajuste (mesure de concordance)\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "\n",
    "ari = adjusted_rand_score(labels_km, labels_cah)\n",
    "nmi = normalized_mutual_info_score(labels_km, labels_cah)\n",
    "\n",
    "print(f\"\\nIndice de Rand ajuste (ARI) : {ari:.3f}\")\n",
    "print(f\"Information mutuelle normalisee (NMI) : {nmi:.3f}\")\n",
    "print(\"\\nInterpretation :\")\n",
    "print(\"  ARI=1 : accord parfait, ARI=0 : assignation aleatoire\")\n",
    "print(\"  NMI=1 : accord parfait, NMI=0 : independance totale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison visuelle (projection PCA)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "cmap = get_cmap('Set2')\n",
    "\n",
    "for ax, (labels, titre) in zip(axes, [\n",
    "    (mat_moyenne['Cluster_KM'].values, 'K-means'),\n",
    "    (mat_moyenne['Cluster_CAH'].values, 'CAH (Ward)')\n",
    "]):\n",
    "    for c in sorted(np.unique(labels)):\n",
    "        mask = labels == c\n",
    "        ax.scatter(X_pca[mask, 0], X_pca[mask, 1], \n",
    "                   c=[cmap((c-1)/k_optimal)], label=f'Cluster {c}', s=120,\n",
    "                   edgecolors='black', linewidth=1, alpha=0.8)\n",
    "    \n",
    "    for i, name in enumerate(mat_moyenne['Entreprise'].values):\n",
    "        ax.annotate(name, (X_pca[i, 0], X_pca[i, 1]),\n",
    "                    textcoords='offset points', xytext=(5, 5), fontsize=7)\n",
    "    \n",
    "    ax.set_xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]*100:.1f}%)')\n",
    "    ax.set_ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]*100:.1f}%)')\n",
    "    ax.set_title(f'{titre} -- Projection ACP 2D')\n",
    "    ax.legend()\n",
    "\n",
    "plt.suptitle('Comparaison K-means vs CAH', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../notebooks/figures/02_comparaison_kmeans_cah.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tableau comparatif des metriques\n",
    "sil_km = silhouette_score(X_scaled, labels_km)\n",
    "ch_km = calinski_harabasz_score(X_scaled, labels_km)\n",
    "db_km = davies_bouldin_score(X_scaled, labels_km)\n",
    "\n",
    "comparaison = pd.DataFrame({\n",
    "    'K-means': [sil_km, ch_km, db_km],\n",
    "    'CAH (Ward)': [sil_cah, ch_cah, db_cah]\n",
    "}, index=['Silhouette (max)', 'Calinski-Harabasz (max)', 'Davies-Bouldin (min)'])\n",
    "\n",
    "print(\"\\nComparaison des metriques de qualite :\")\n",
    "print(comparaison.round(3).to_string())\n",
    "\n",
    "# Identification de la meilleure methode par metrique\n",
    "print(\"\\nMeilleure methode par critere :\")\n",
    "print(f\"  Silhouette        : {'K-means' if sil_km >= sil_cah else 'CAH'}\")\n",
    "print(f\"  Calinski-Harabasz : {'K-means' if ch_km >= ch_cah else 'CAH'}\")\n",
    "print(f\"  Davies-Bouldin    : {'K-means' if db_km <= db_cah else 'CAH'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Analyse des clusters sur les variations annuelles\n",
    "\n",
    "Comme dans le paper, on applique egalement le clustering sur les variations annuelles \n",
    "plutot que sur les niveaux. Cela permet de regrouper les entreprises non pas par leur \n",
    "score absolu, mais par leur dynamique d'amelioration/degradation ESG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation de la matrice de variations moyennes\n",
    "score_cols_env = [c for c in df_env_var.columns if c != 'Date']\n",
    "score_cols_soc = [c for c in df_soc_var.columns if c != 'Date']\n",
    "score_cols_esg = [c for c in df_esg_var.columns if c != 'Date']\n",
    "\n",
    "mat_var = pd.DataFrame({\n",
    "    'Env_Var': df_env_var[score_cols_env].mean(),\n",
    "    'Soc_Var': df_soc_var[score_cols_soc].mean(),\n",
    "    'ESG_Var': df_esg_var[score_cols_esg].mean()\n",
    "})\n",
    "mat_var['Entreprise'] = mat_var.index.map(TICKER_NAMES)\n",
    "mat_var = mat_var.dropna(thresh=3)\n",
    "\n",
    "# Standardisation\n",
    "features_var = ['Env_Var', 'Soc_Var', 'ESG_Var']\n",
    "scaler_var = StandardScaler()\n",
    "X_var_scaled = scaler_var.fit_transform(mat_var[features_var])\n",
    "\n",
    "print(f\"Matrice de variations : {mat_var.shape[0]} entreprises x 3 variables\")\n",
    "mat_var[features_var + ['Entreprise']].round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-means sur les variations\n",
    "K_range_var = range(2, min(8, len(X_var_scaled)))\n",
    "sil_var = []\n",
    "for k in K_range_var:\n",
    "    km_v = KMeans(n_clusters=k, n_init=50, random_state=42)\n",
    "    labels_v = km_v.fit_predict(X_var_scaled)\n",
    "    sil_var.append(silhouette_score(X_var_scaled, labels_v))\n",
    "\n",
    "k_opt_var = list(K_range_var)[np.argmax(sil_var)]\n",
    "print(f\"Nombre optimal de clusters (variations) : k = {k_opt_var} (silhouette = {max(sil_var):.3f})\")\n",
    "\n",
    "# Application\n",
    "km_var = KMeans(n_clusters=k_opt_var, n_init=50, random_state=42)\n",
    "mat_var['Cluster_Var_KM'] = km_var.fit_predict(X_var_scaled) + 1\n",
    "\n",
    "# CAH sur les variations\n",
    "Z_var = linkage(X_var_scaled, method='ward')\n",
    "mat_var['Cluster_Var_CAH'] = fcluster(Z_var, t=k_opt_var, criterion='maxclust')\n",
    "\n",
    "# Dendrogramme\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "short_labels_var = [TICKER_NAMES.get(t, t)[:12] for t in mat_var.index]\n",
    "dendrogram(Z_var, labels=short_labels_var, leaf_rotation=45, leaf_font_size=9, ax=ax)\n",
    "ax.set_ylabel('Distance (Ward)')\n",
    "ax.set_title('Dendrogramme CAH sur les variations annuelles des scores ESG')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../notebooks/figures/02_dendrogramme_variations.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Composition des clusters (variations)\n",
    "print(f\"\\n=== Clusters sur les variations annuelles (K-means, k={k_opt_var}) ===\\n\")\n",
    "for c in sorted(mat_var['Cluster_Var_KM'].unique()):\n",
    "    members = mat_var[mat_var['Cluster_Var_KM'] == c]\n",
    "    print(f\"--- Cluster {c} ({len(members)} entreprises) ---\")\n",
    "    for _, row in members.iterrows():\n",
    "        print(f\"  {row['Entreprise']:20s}  dEnv={row['Env_Var']:+.3f}  dSoc={row['Soc_Var']:+.3f}  dESG={row['ESG_Var']:+.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Stabilite du K-means (sensibilite a l'initialisation)\n",
    "\n",
    "Le K-means est sensible a l'initialisation des centroides. Pour evaluer la stabilite \n",
    "de nos resultats, on execute l'algorithme 100 fois avec des initialisations differentes \n",
    "et on mesure la concordance des partitions obtenues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de stabilite\n",
    "n_runs = 100\n",
    "ari_scores = []\n",
    "ref_labels = KMeans(n_clusters=k_optimal, n_init=50, random_state=42).fit_predict(X_scaled)\n",
    "\n",
    "for seed in range(n_runs):\n",
    "    km_test = KMeans(n_clusters=k_optimal, n_init=1, random_state=seed)\n",
    "    test_labels = km_test.fit_predict(X_scaled)\n",
    "    ari_scores.append(adjusted_rand_score(ref_labels, test_labels))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.hist(ari_scores, bins=20, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "ax.axvline(x=np.mean(ari_scores), color='red', linestyle='--', linewidth=2,\n",
    "           label=f'ARI moyen = {np.mean(ari_scores):.3f}')\n",
    "ax.set_xlabel('Adjusted Rand Index')\n",
    "ax.set_ylabel('Frequence')\n",
    "ax.set_title(f'Stabilite du K-means (k={k_optimal}) -- {n_runs} initialisations')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../notebooks/figures/02_stabilite_kmeans.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"ARI moyen : {np.mean(ari_scores):.3f} (+/- {np.std(ari_scores):.3f})\")\n",
    "print(f\"Proportion de runs avec ARI > 0.9 : {100 * np.mean(np.array(ari_scores) > 0.9):.0f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Sauvegarde des resultats\n",
    "\n",
    "On sauvegarde les affectations de cluster pour les reutiliser dans le Notebook 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('../data/clean', exist_ok=True)\n",
    "\n",
    "# Sauvegarde des resultats de clustering (niveaux)\n",
    "mat_moyenne.to_csv('../data/clean/clustering_niveaux.csv')\n",
    "\n",
    "# Sauvegarde des resultats de clustering (variations)\n",
    "mat_var.to_csv('../data/clean/clustering_variations.csv')\n",
    "\n",
    "# Sauvegarde des donnees standardisees\n",
    "X_scaled_df.to_csv('../data/clean/scores_standardises.csv')\n",
    "\n",
    "print(\"Resultats sauvegardes dans data/clean/\")\n",
    "print(\"  - clustering_niveaux.csv\")\n",
    "print(\"  - clustering_variations.csv\")\n",
    "print(\"  - scores_standardises.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Synthese du Notebook 2\n",
    "\n",
    "### Methodes appliquees\n",
    "- **K-means** avec determination du k optimal via silhouette, Calinski-Harabasz et Davies-Bouldin\n",
    "- **CAH** avec comparaison de 4 criteres de lien (Ward, Complete, Average, Single)\n",
    "- Clustering sur **niveaux absolus** et sur **variations annuelles**\n",
    "\n",
    "### Principaux resultats\n",
    "- Le nombre optimal de clusters et les metriques de qualite seront visibles a l'execution\n",
    "- La concordance K-means/CAH (ARI) permet de juger de la robustesse de la structure identifiee\n",
    "- L'analyse de stabilite du K-means montre la sensibilite a l'initialisation\n",
    "\n",
    "### Limites identifiees (a adresser dans le Notebook 3)\n",
    "- K-means suppose des clusters spheriques (hypothese forte)\n",
    "- Pas de reduction de dimension prealable (ACP)\n",
    "- Distance euclidienne uniquement\n",
    "- Pas d'analyse temporelle (les clusters sont statiques)\n",
    "- Pas de methodes alternatives (DBSCAN, GMM)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}