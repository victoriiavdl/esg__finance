{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1 -- Nettoyage des donnees et analyse exploratoire\n",
    "\n",
    "## Contexte\n",
    "\n",
    "Ce notebook constitue la premiere etape de notre analyse des scores ESG (Environmental, Social, Governance) \n",
    "pour un panel de 23 entreprises technologiques et innovantes cotees en bourse. Les donnees proviennent \n",
    "du terminal Bloomberg (scores BESG) et couvrent la periode 2016-2026 avec une frequence mensuelle.\n",
    "\n",
    "L'objectif est de reproduire et d'ameliorer la methodologie presentee dans le paper de reference \n",
    "(\"K-means et analyse de clustering hierarchique agglomeratif des scores ESG\") en l'appliquant \n",
    "a un univers d'investissement different : les entreprises liees a l'intelligence artificielle et \n",
    "aux semi-conducteurs, plutot que le secteur de l'energie.\n",
    "\n",
    "## Structure de ce notebook\n",
    "\n",
    "1. Chargement et inspection des donnees brutes\n",
    "2. Nettoyage : conversion des formats (dates Excel, separateurs decimaux, valeurs manquantes)\n",
    "3. Restructuration en format exploitable (panel long)\n",
    "4. Statistiques descriptives\n",
    "5. Analyse des valeurs manquantes\n",
    "6. Visualisations exploratoires (evolution temporelle, distributions, correlations)\n",
    "\n",
    "## Fichiers sources\n",
    "\n",
    "- `data/ENVIRONMENTAL_SCORE.csv` : BESG Environmental Pillar Score\n",
    "- `data/SOCIAL_SCORE.csv` : BESG Social Pillar Score\n",
    "- `data/esg score.csv` : BESG ESG Score (score composite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Imports et configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T13:55:30.681008Z",
     "start_time": "2026-02-15T13:55:30.678970Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration graphique\n",
    "plt.rcParams['figure.figsize'] = (14, 7)\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['grid.alpha'] = 0.3\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Chemin vers les donnees\n",
    "DATA_DIR = '../data/'\n",
    "\n",
    "print('Imports effectues avec succes.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Chargement des donnees brutes\n",
    "\n",
    "Les fichiers CSV exportes depuis Bloomberg ont un format particulier :\n",
    "- Les 10 premieres lignes contiennent des metadonnees (titre, dates de debut/fin, tickers, noms de champs)\n",
    "- Les donnees effectives commencent a la ligne 11\n",
    "- La premiere colonne est vide, la deuxieme contient les dates au format numerique Excel\n",
    "- Les scores utilisent la virgule comme separateur decimal (format francais)\n",
    "- Les valeurs manquantes sont codees `#N/A N/A`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T13:55:30.770484Z",
     "start_time": "2026-02-15T13:55:30.751345Z"
    }
   },
   "outputs": [],
   "source": [
    "def charger_csv_bloomberg(filepath, nom_score):\n",
    "    \"\"\"\n",
    "    Charge un fichier CSV exporte depuis Bloomberg et le nettoie.\n",
    "    \n",
    "    Parametres\n",
    "    ----------\n",
    "    filepath : str\n",
    "        Chemin vers le fichier CSV.\n",
    "    nom_score : str\n",
    "        Nom du type de score (ex: 'Environmental', 'Social', 'ESG').\n",
    "    \n",
    "    Retourne\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame nettoye en format long avec colonnes : Date, Ticker, Score, Type.\n",
    "    \"\"\"\n",
    "    # Lecture brute : on saute les 9 premieres lignes de metadonnees\n",
    "    # La ligne 10 (index 9) contient 'Dates' et les noms de colonnes\n",
    "    raw = pd.read_csv(\n",
    "        filepath,\n",
    "        header=None,\n",
    "        skiprows=10,\n",
    "        na_values=['#N/A N/A', '#N/A', 'N/A', ''],\n",
    "        encoding='utf-8'\n",
    "    )\n",
    "    \n",
    "    # Lecture de la ligne d'en-tete avec les tickers (ligne 8, index 7)\n",
    "    header_raw = pd.read_csv(filepath, header=None, nrows=10, encoding='utf-8')\n",
    "    tickers = header_raw.iloc[7, 2:].values  # Les tickers commencent a la colonne 2\n",
    "    \n",
    "    # Construction du DataFrame\n",
    "    # Colonne 0 : vide, Colonne 1 : Dates, Colonnes 2+ : scores par entreprise\n",
    "    df = raw.iloc[:, 1:].copy()\n",
    "    df.columns = ['Date_Excel'] + list(tickers)\n",
    "    \n",
    "    print(f\"[{nom_score}] Dimensions brutes : {df.shape[0]} lignes x {df.shape[1]-1} entreprises\")\n",
    "    print(f\"[{nom_score}] Periode : date Excel {df['Date_Excel'].iloc[0]} a {df['Date_Excel'].iloc[-1]}\")\n",
    "    \n",
    "    return df, tickers\n",
    "\n",
    "\n",
    "# Chargement des trois fichiers\n",
    "df_env_raw, tickers_env = charger_csv_bloomberg(DATA_DIR + 'ENVIRONMENTAL_SCORE.csv', 'Environmental')\n",
    "df_soc_raw, tickers_soc = charger_csv_bloomberg(DATA_DIR + 'SOCIAL_SCORE.csv', 'Social')\n",
    "df_esg_raw, tickers_esg = charger_csv_bloomberg(DATA_DIR + 'esg score.csv', 'ESG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T13:55:30.778230Z",
     "start_time": "2026-02-15T13:55:30.775600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Verification : les tickers sont-ils identiques dans les 3 fichiers ?\n",
    "print(\"Tickers Environmental :\", list(tickers_env))\n",
    "print(\"\\nTickers Social :\", list(tickers_soc))\n",
    "print(\"\\nTickers ESG :\", list(tickers_esg))\n",
    "print(\"\\nTickers identiques dans les 3 fichiers :\", \n",
    "      np.array_equal(tickers_env, tickers_soc) and np.array_equal(tickers_soc, tickers_esg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T13:55:30.802933Z",
     "start_time": "2026-02-15T13:55:30.787094Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apercu des donnees brutes (avant nettoyage)\n",
    "print(\"=== Apercu Environmental Score (brut) ===\")\n",
    "df_env_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Nettoyage des donnees\n",
    "\n",
    "### 3.1 Conversion des dates\n",
    "\n",
    "Les dates sont au format numerique Excel (nombre de jours depuis le 30 decembre 1899). \n",
    "Par exemple, 42369 correspond au 1er janvier 2016.\n",
    "\n",
    "### 3.2 Conversion des separateurs decimaux\n",
    "\n",
    "Les scores utilisent la virgule comme separateur decimal (ex: `\"3,43\"` pour 3.43). \n",
    "Il faut remplacer les virgules par des points puis convertir en float.\n",
    "\n",
    "### 3.3 Traitement des valeurs manquantes\n",
    "\n",
    "Les `#N/A N/A` de Bloomberg indiquent que le score n'etait pas encore disponible pour cette entreprise \n",
    "a cette date. C'est un phenomene courant : les scores ESG ne sont pas calcules des l'introduction \n",
    "en bourse, ils apparaissent progressivement au fil du temps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T13:55:30.852459Z",
     "start_time": "2026-02-15T13:55:30.827038Z"
    }
   },
   "outputs": [],
   "source": [
    "def excel_date_to_datetime(excel_date):\n",
    "    \"\"\"\n",
    "    Convertit une date au format numerique Excel en datetime Python.\n",
    "    La base Excel est le 30 decembre 1899 (convention Windows).\n",
    "    \"\"\"\n",
    "    base = datetime(1899, 12, 30)\n",
    "    return base + timedelta(days=int(excel_date))\n",
    "\n",
    "\n",
    "def nettoyer_scores(df, nom_score):\n",
    "    \"\"\"\n",
    "    Nettoie un DataFrame brut de scores Bloomberg :\n",
    "    - Convertit les dates Excel en datetime\n",
    "    - Remplace les virgules par des points dans les scores\n",
    "    - Convertit les scores en float\n",
    "    - Gere les valeurs manquantes\n",
    "    \n",
    "    Parametres\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame brut avec Date_Excel + colonnes de scores.\n",
    "    nom_score : str\n",
    "        Type de score pour le nommage.\n",
    "    \n",
    "    Retourne\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame nettoye avec dates converties et scores en float.\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Conversion des dates\n",
    "    df_clean['Date'] = df_clean['Date_Excel'].apply(excel_date_to_datetime)\n",
    "    df_clean = df_clean.drop(columns=['Date_Excel'])\n",
    "    \n",
    "    # Conversion des scores : remplacer virgule par point puis convertir en float\n",
    "    score_cols = [c for c in df_clean.columns if c != 'Date']\n",
    "    for col in score_cols:\n",
    "        # Certaines valeurs sont deja numeriques (int ou float), d'autres sont des strings\n",
    "        df_clean[col] = df_clean[col].apply(\n",
    "            lambda x: float(str(x).replace(',', '.')) if pd.notna(x) and str(x).strip() != '' else np.nan\n",
    "        )\n",
    "    \n",
    "    # Reordonner les colonnes\n",
    "    df_clean = df_clean[['Date'] + score_cols]\n",
    "    \n",
    "    # Statistiques de nettoyage\n",
    "    n_total = df_clean[score_cols].size\n",
    "    n_missing = df_clean[score_cols].isna().sum().sum()\n",
    "    pct_missing = 100 * n_missing / n_total\n",
    "    print(f\"[{nom_score}] Apres nettoyage : {n_missing}/{n_total} valeurs manquantes ({pct_missing:.1f}%)\")\n",
    "    print(f\"[{nom_score}] Periode : {df_clean['Date'].min().strftime('%Y-%m-%d')} a {df_clean['Date'].max().strftime('%Y-%m-%d')}\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "\n",
    "# Application du nettoyage\n",
    "df_env = nettoyer_scores(df_env_raw, 'Environmental')\n",
    "df_soc = nettoyer_scores(df_soc_raw, 'Social')\n",
    "df_esg = nettoyer_scores(df_esg_raw, 'ESG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T13:55:30.886429Z",
     "start_time": "2026-02-15T13:55:30.878987Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apercu apres nettoyage\n",
    "print(\"=== Environmental Score (nettoye) ===\")\n",
    "df_env.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T13:55:31.203030Z",
     "start_time": "2026-02-15T13:55:31.195556Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"=== Social Score (nettoye) ===\")\n",
    "df_soc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T13:55:31.342243Z",
     "start_time": "2026-02-15T13:55:31.334540Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"=== ESG Score global (nettoye) ===\")\n",
    "df_esg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Restructuration en format long (panel)\n",
    "\n",
    "Pour faciliter les analyses statistiques et les visualisations, on transforme les DataFrames \n",
    "du format large (une colonne par entreprise) au format long (une ligne par observation date/entreprise). \n",
    "On fusionne ensuite les trois types de scores dans un seul DataFrame panel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T13:55:31.567082Z",
     "start_time": "2026-02-15T13:55:31.549384Z"
    }
   },
   "outputs": [],
   "source": [
    "def wide_to_long(df, score_type):\n",
    "    \"\"\"\n",
    "    Transforme un DataFrame wide en format long.\n",
    "    \n",
    "    Parametres\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame en format large (Date + colonnes de scores par ticker).\n",
    "    score_type : str\n",
    "        Nom du type de score.\n",
    "    \n",
    "    Retourne\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Format long avec colonnes : Date, Ticker, Score.\n",
    "    \"\"\"\n",
    "    score_cols = [c for c in df.columns if c != 'Date']\n",
    "    df_long = df.melt(id_vars='Date', value_vars=score_cols,\n",
    "                      var_name='Ticker', value_name=score_type)\n",
    "    return df_long\n",
    "\n",
    "\n",
    "# Transformation en format long\n",
    "df_env_long = wide_to_long(df_env, 'Env_Score')\n",
    "df_soc_long = wide_to_long(df_soc, 'Soc_Score')\n",
    "df_esg_long = wide_to_long(df_esg, 'ESG_Score')\n",
    "\n",
    "# Fusion des trois scores dans un seul DataFrame panel\n",
    "df_panel = df_env_long.merge(df_soc_long, on=['Date', 'Ticker'], how='outer')\n",
    "df_panel = df_panel.merge(df_esg_long, on=['Date', 'Ticker'], how='outer')\n",
    "\n",
    "# Tri par ticker puis par date\n",
    "df_panel = df_panel.sort_values(['Ticker', 'Date']).reset_index(drop=True)\n",
    "\n",
    "print(f\"Dimensions du panel : {df_panel.shape}\")\n",
    "print(f\"Nombre d'entreprises : {df_panel['Ticker'].nunique()}\")\n",
    "print(f\"Nombre de dates : {df_panel['Date'].nunique()}\")\n",
    "df_panel.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T13:55:31.619370Z",
     "start_time": "2026-02-15T13:55:31.616096Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dictionnaire pour identifier les entreprises\n",
    "TICKER_NAMES = {\n",
    "    'NVDA US Equity': 'NVIDIA',\n",
    "    'AVGO US Equity': 'Broadcom',\n",
    "    'TSM US Equity': 'TSMC',\n",
    "    'GOOGL US Equity': 'Alphabet (Google)',\n",
    "    '000660 KS Equity': 'SK Hynix',\n",
    "    'LRCX US Equity': 'Lam Research',\n",
    "    '6857 JP Equity': 'Advantest',\n",
    "    'TSEM IT Equity': 'Tower Semiconductor',\n",
    "    'MSFT US Equity': 'Microsoft',\n",
    "    'LITE US Equity': 'Lumentum',\n",
    "    'AMD US Equity': 'AMD',\n",
    "    'FN US Equity': 'Fabrinet',\n",
    "    'SNOW US Equity': 'Snowflake',\n",
    "    'MU US Equity': 'Micron Technology',\n",
    "    'TSLA US Equity': 'Tesla',\n",
    "    '9984 JP Equity': 'SoftBank',\n",
    "    'CRDO US Equity': 'CREDO Technology',\n",
    "    'ENR GR Equity': 'Siemens Energy',\n",
    "    'MPWR US Equity': 'Monolithic Power Systems',\n",
    "    '2383 TT Equity': 'Elite Material',\n",
    "    'CLS CN Equity': 'Celestica',\n",
    "    'META US Equity': 'Meta Platforms',\n",
    "    'AMZN US Equity': 'Amazon'\n",
    "}\n",
    "\n",
    "df_panel['Entreprise'] = df_panel['Ticker'].map(TICKER_NAMES)\n",
    "print(\"Entreprises du panel :\")\n",
    "for ticker, name in sorted(TICKER_NAMES.items(), key=lambda x: x[1]):\n",
    "    print(f\"  {ticker:25s} -> {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Analyse des valeurs manquantes\n",
    "\n",
    "L'analyse des valeurs manquantes est fondamentale avant toute modelisation. \n",
    "Dans le contexte des scores ESG Bloomberg, les valeurs manquantes ne sont generalement pas \n",
    "aleatoires (MCAR) mais structurelles : elles correspondent a des periodes ou l'entreprise \n",
    "n'avait pas encore de score BESG attribue.\n",
    "\n",
    "On distingue :\n",
    "- Les **manquantes en debut de serie** (left-censoring) : le score n'existait pas encore\n",
    "- Les **manquantes sporadiques** : potentiellement des erreurs ou des periodes de recalcul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T13:55:32.585243Z",
     "start_time": "2026-02-15T13:55:31.669653Z"
    }
   },
   "outputs": [],
   "source": [
    "# Matrice de valeurs manquantes pour chaque type de score (format wide)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 8))\n",
    "\n",
    "for ax, (df_w, titre) in zip(axes, [\n",
    "    (df_env, 'Environmental Score'),\n",
    "    (df_soc, 'Social Score'),\n",
    "    (df_esg, 'ESG Score Global')\n",
    "]):\n",
    "    score_cols = [c for c in df_w.columns if c != 'Date']\n",
    "    # Matrice binaire : 1 = present, 0 = manquant\n",
    "    presence = df_w[score_cols].notna().astype(int)\n",
    "    # Renommer les colonnes pour lisibilite\n",
    "    presence.columns = [TICKER_NAMES.get(c, c).split('(')[0].strip()[:12] for c in presence.columns]\n",
    "    \n",
    "    sns.heatmap(presence.T, cmap='YlGn', cbar_kws={'label': 'Donnee presente'},\n",
    "                xticklabels=20, ax=ax)\n",
    "    ax.set_xlabel('Indice temporel')\n",
    "    ax.set_ylabel('')\n",
    "    ax.set_title(f'Disponibilite -- {titre}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../notebooks/figures/01_missing_data_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Figure sauvegardee : figures/01_missing_data_heatmap.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T13:56:28.137158Z",
     "start_time": "2026-02-15T13:56:28.120964Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pourcentage de valeurs manquantes par entreprise\n",
    "missing_summary = pd.DataFrame()\n",
    "for df_w, nom in [(df_env, 'Env'), (df_soc, 'Soc'), (df_esg, 'ESG')]:\n",
    "    score_cols = [c for c in df_w.columns if c != 'Date']\n",
    "    pct = df_w[score_cols].isna().mean() * 100\n",
    "    missing_summary[nom] = pct\n",
    "\n",
    "missing_summary.index = [TICKER_NAMES.get(c, c) for c in missing_summary.index]\n",
    "missing_summary['Moyenne'] = missing_summary.mean(axis=1)\n",
    "missing_summary = missing_summary.sort_values('Moyenne', ascending=False)\n",
    "\n",
    "print(\"Pourcentage de valeurs manquantes par entreprise (%) :\")\n",
    "print(missing_summary.round(1).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T13:56:31.164841Z",
     "start_time": "2026-02-15T13:56:31.148850Z"
    }
   },
   "outputs": [],
   "source": [
    "# Date de premiere apparition du score pour chaque entreprise\n",
    "print(\"\\nDate de premiere observation disponible par entreprise et par score :\\n\")\n",
    "first_dates = pd.DataFrame()\n",
    "for df_w, nom in [(df_env, 'Env'), (df_soc, 'Soc'), (df_esg, 'ESG')]:\n",
    "    score_cols = [c for c in df_w.columns if c != 'Date']\n",
    "    first = {}\n",
    "    for col in score_cols:\n",
    "        valid = df_w.loc[df_w[col].notna(), 'Date']\n",
    "        first[col] = valid.iloc[0].strftime('%Y-%m') if len(valid) > 0 else 'N/A'\n",
    "    first_dates[nom] = pd.Series(first)\n",
    "\n",
    "first_dates.index = [TICKER_NAMES.get(c, c) for c in first_dates.index]\n",
    "print(first_dates.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Statistiques descriptives\n",
    "\n",
    "On calcule les statistiques descriptives classiques (moyenne, ecart-type, min, max, quartiles) \n",
    "pour chaque type de score. Cette etape permet de detecter d'eventuelles anomalies \n",
    "(scores hors echelle, distributions atypiques) et de mieux comprendre la structure des donnees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T13:56:33.868325Z",
     "start_time": "2026-02-15T13:56:33.857939Z"
    }
   },
   "outputs": [],
   "source": [
    "# Statistiques descriptives globales\n",
    "print(\"=\" * 60)\n",
    "print(\"STATISTIQUES DESCRIPTIVES DU PANEL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for score_col, titre in [('Env_Score', 'Environmental'), ('Soc_Score', 'Social'), ('ESG_Score', 'ESG Global')]:\n",
    "    print(f\"\\n--- {titre} ---\")\n",
    "    print(df_panel[score_col].describe().round(3).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T13:56:37.522390Z",
     "start_time": "2026-02-15T13:56:37.511908Z"
    }
   },
   "outputs": [],
   "source": [
    "# Statistiques descriptives par entreprise (derniere observation disponible)\n",
    "# On prend le dernier score disponible pour chaque entreprise comme proxy de l'etat actuel\n",
    "latest_scores = df_panel.groupby('Ticker').last().reset_index()\n",
    "latest_scores['Entreprise'] = latest_scores['Ticker'].map(TICKER_NAMES)\n",
    "\n",
    "summary_table = latest_scores[['Entreprise', 'Env_Score', 'Soc_Score', 'ESG_Score']].copy()\n",
    "summary_table = summary_table.sort_values('ESG_Score', ascending=False).reset_index(drop=True)\n",
    "summary_table.index = summary_table.index + 1  # Index a partir de 1\n",
    "\n",
    "print(\"\\nClassement des entreprises par score ESG global (derniere observation) :\")\n",
    "print(summary_table.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T13:57:06.081543Z",
     "start_time": "2026-02-15T13:56:59.837026Z"
    }
   },
   "outputs": [],
   "source": [
    "# Distribution des scores (histogrammes + KDE)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for ax, (col, titre, couleur) in zip(axes, [\n",
    "    ('Env_Score', 'Environmental Score', '#2ca02c'),\n",
    "    ('Soc_Score', 'Social Score', '#1f77b4'),\n",
    "    ('ESG_Score', 'ESG Score Global', '#d62728')\n",
    "]):\n",
    "    data = df_panel[col].dropna()\n",
    "    ax.hist(data, bins=30, density=True, alpha=0.6, color=couleur, edgecolor='white')\n",
    "    data.plot.kde(ax=ax, color=couleur, linewidth=2)\n",
    "    ax.axvline(data.mean(), color='black', linestyle='--', linewidth=1.5, label=f'Moyenne = {data.mean():.2f}')\n",
    "    ax.axvline(data.median(), color='gray', linestyle=':', linewidth=1.5, label=f'Mediane = {data.median():.2f}')\n",
    "    ax.set_xlabel('Score')\n",
    "    ax.set_ylabel('Densite')\n",
    "    ax.set_title(f'Distribution -- {titre}')\n",
    "    ax.legend(fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../notebooks/figures/01_distributions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Visualisation de l'evolution temporelle\n",
    "\n",
    "L'evolution temporelle des scores ESG permet de detecter des tendances structurelles \n",
    "(amelioration ou degradation) ainsi que des ruptures potentielles \n",
    "(changement de methodologie Bloomberg, evenement externe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evolution temporelle de toutes les entreprises pour chaque score\n",
    "fig, axes = plt.subplots(3, 1, figsize=(16, 18))\n",
    "\n",
    "for ax, (df_w, titre, cmap_name) in zip(axes, [\n",
    "    (df_env, 'Environmental Score', 'Greens'),\n",
    "    (df_soc, 'Social Score', 'Blues'),\n",
    "    (df_esg, 'ESG Score Global', 'Reds')\n",
    "]):\n",
    "    score_cols = [c for c in df_w.columns if c != 'Date']\n",
    "    cmap = plt.cm.get_cmap(cmap_name, len(score_cols))\n",
    "    \n",
    "    for i, col in enumerate(score_cols):\n",
    "        short_name = TICKER_NAMES.get(col, col).split('(')[0].strip()\n",
    "        ax.plot(df_w['Date'], df_w[col], label=short_name, \n",
    "                color=cmap(i / len(score_cols)), alpha=0.8, linewidth=1.2)\n",
    "    \n",
    "    ax.set_ylabel('Score BESG')\n",
    "    ax.set_title(f'Evolution temporelle -- {titre}')\n",
    "    ax.legend(bbox_to_anchor=(1.02, 1), loc='upper left', fontsize=7, ncol=2)\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "    ax.xaxis.set_major_locator(mdates.YearLocator())\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../notebooks/figures/01_evolution_temporelle.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evolution de la moyenne et de l'ecart-type du panel au cours du temps\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "for df_w, titre, color in [\n",
    "    (df_env, 'Environmental', '#2ca02c'),\n",
    "    (df_soc, 'Social', '#1f77b4'),\n",
    "    (df_esg, 'ESG Global', '#d62728')\n",
    "]:\n",
    "    score_cols = [c for c in df_w.columns if c != 'Date']\n",
    "    mean_ts = df_w[score_cols].mean(axis=1)\n",
    "    std_ts = df_w[score_cols].std(axis=1)\n",
    "    \n",
    "    axes[0].plot(df_w['Date'], mean_ts, label=titre, color=color, linewidth=2)\n",
    "    axes[0].fill_between(df_w['Date'], mean_ts - std_ts, mean_ts + std_ts, \n",
    "                          alpha=0.15, color=color)\n",
    "    axes[1].plot(df_w['Date'], std_ts, label=titre, color=color, linewidth=2)\n",
    "\n",
    "axes[0].set_ylabel('Score moyen du panel')\n",
    "axes[0].set_title('Moyenne des scores ESG du panel (bande = +/- 1 ecart-type)')\n",
    "axes[0].legend()\n",
    "axes[0].xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "\n",
    "axes[1].set_ylabel('Ecart-type')\n",
    "axes[1].set_title('Dispersion (ecart-type) des scores au sein du panel')\n",
    "axes[1].legend()\n",
    "axes[1].xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../notebooks/figures/01_mean_std_evolution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Analyse des correlations\n",
    "\n",
    "L'etude des correlations entre les trois piliers ESG permet de verifier si les scores \n",
    "sont redondants (forte correlation) ou apportent chacun une information distincte. \n",
    "Cela a des implications directes pour la strategie de clustering : si les piliers sont \n",
    "tres correles, une reduction de dimension (ACP) sera pertinente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation entre les trois types de scores\n",
    "corr_scores = df_panel[['Env_Score', 'Soc_Score', 'ESG_Score']].corr()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "sns.heatmap(corr_scores, annot=True, fmt='.3f', cmap='RdYlBu_r', center=0,\n",
    "            vmin=-1, vmax=1, square=True, ax=ax,\n",
    "            xticklabels=['Environmental', 'Social', 'ESG Global'],\n",
    "            yticklabels=['Environmental', 'Social', 'ESG Global'])\n",
    "ax.set_title('Matrice de correlation entre les piliers ESG')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../notebooks/figures/01_correlation_piliers.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nMatrice de correlation :\")\n",
    "print(corr_scores.round(4).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plots entre les piliers\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "pairs = [\n",
    "    ('Env_Score', 'Soc_Score', 'Environnemental vs Social'),\n",
    "    ('Env_Score', 'ESG_Score', 'Environnemental vs ESG Global'),\n",
    "    ('Soc_Score', 'ESG_Score', 'Social vs ESG Global')\n",
    "]\n",
    "\n",
    "for ax, (x_col, y_col, title) in zip(axes, pairs):\n",
    "    subset = df_panel.dropna(subset=[x_col, y_col])\n",
    "    ax.scatter(subset[x_col], subset[y_col], alpha=0.3, s=10, color='steelblue')\n",
    "    \n",
    "    # Droite de regression\n",
    "    z = np.polyfit(subset[x_col], subset[y_col], 1)\n",
    "    p = np.poly1d(z)\n",
    "    x_range = np.linspace(subset[x_col].min(), subset[x_col].max(), 100)\n",
    "    ax.plot(x_range, p(x_range), 'r--', linewidth=2, \n",
    "            label=f'y = {z[0]:.2f}x + {z[1]:.2f}')\n",
    "    \n",
    "    ax.set_xlabel(x_col.replace('_', ' '))\n",
    "    ax.set_ylabel(y_col.replace('_', ' '))\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../notebooks/figures/01_scatter_piliers.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation inter-entreprises (sur le score ESG global)\n",
    "# Utilise le format wide : une colonne par entreprise\n",
    "score_cols = [c for c in df_esg.columns if c != 'Date']\n",
    "corr_entreprises = df_esg[score_cols].corr()\n",
    "\n",
    "# Renommer pour lisibilite\n",
    "short_names = [TICKER_NAMES.get(c, c).split('(')[0].strip()[:10] for c in corr_entreprises.columns]\n",
    "corr_entreprises.columns = short_names\n",
    "corr_entreprises.index = short_names\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 12))\n",
    "mask = np.triu(np.ones_like(corr_entreprises, dtype=bool), k=1)\n",
    "sns.heatmap(corr_entreprises, mask=mask, annot=True, fmt='.2f', cmap='RdYlBu_r',\n",
    "            center=0, vmin=-1, vmax=1, ax=ax, annot_kws={'size': 7})\n",
    "ax.set_title('Matrice de correlation inter-entreprises (Score ESG Global)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../notebooks/figures/01_correlation_entreprises.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Calcul des variations annuelles\n",
    "\n",
    "Comme dans le paper de reference, on calcule les variations annuelles des scores ESG. \n",
    "Ces variations capturent la dynamique d'amelioration ou de degradation des pratiques ESG, \n",
    "ce qui est une information complementaire au niveau absolu du score.\n",
    "\n",
    "On calcule la variation en difference absolue (delta) plutot qu'en pourcentage, \n",
    "car les scores BESG sont deja sur une echelle bornee [0, 10]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculer_variations_annuelles(df_wide):\n",
    "    \"\"\"\n",
    "    Calcule les variations annuelles (delta) pour chaque entreprise.\n",
    "    On prend la difference entre le score de la meme periode d'une annee a l'autre (lag 12 mois).\n",
    "    \n",
    "    Parametres\n",
    "    ----------\n",
    "    df_wide : pd.DataFrame\n",
    "        DataFrame en format wide avec colonne 'Date' et colonnes de scores.\n",
    "    \n",
    "    Retourne\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame des variations annuelles, meme format.\n",
    "    \"\"\"\n",
    "    score_cols = [c for c in df_wide.columns if c != 'Date']\n",
    "    df_var = df_wide.copy()\n",
    "    \n",
    "    for col in score_cols:\n",
    "        df_var[col] = df_wide[col].diff(periods=12)  # Diff sur 12 mois\n",
    "    \n",
    "    return df_var\n",
    "\n",
    "\n",
    "df_env_var = calculer_variations_annuelles(df_env)\n",
    "df_soc_var = calculer_variations_annuelles(df_soc)\n",
    "df_esg_var = calculer_variations_annuelles(df_esg)\n",
    "\n",
    "# Statistiques des variations\n",
    "print(\"Statistiques des variations annuelles du score ESG Global :\")\n",
    "score_cols = [c for c in df_esg_var.columns if c != 'Date']\n",
    "var_stats = df_esg_var[score_cols].describe()\n",
    "var_stats.columns = [TICKER_NAMES.get(c, c).split('(')[0].strip()[:12] for c in var_stats.columns]\n",
    "print(var_stats.round(3).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot des variations annuelles par entreprise\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "\n",
    "score_cols = [c for c in df_esg_var.columns if c != 'Date']\n",
    "var_data = df_esg_var[score_cols].dropna()\n",
    "var_data.columns = [TICKER_NAMES.get(c, c).split('(')[0].strip()[:10] for c in var_data.columns]\n",
    "\n",
    "var_data.boxplot(ax=ax, vert=True, patch_artist=True,\n",
    "                 boxprops=dict(facecolor='lightsteelblue', color='navy'),\n",
    "                 medianprops=dict(color='red', linewidth=2))\n",
    "ax.axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
    "ax.set_ylabel('Variation annuelle du score ESG')\n",
    "ax.set_title('Distribution des variations annuelles du score ESG par entreprise')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../notebooks/figures/01_variations_boxplot.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Sauvegarde des donnees nettoyees\n",
    "\n",
    "On sauvegarde les donnees nettoyees en format CSV pour les reutiliser dans les notebooks suivants. \n",
    "Cela evite de repeter le processus de nettoyage et garantit la coherence entre les etapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Creation du repertoire de sortie\n",
    "os.makedirs('../data/clean', exist_ok=True)\n",
    "os.makedirs('../notebooks/figures', exist_ok=True)\n",
    "\n",
    "# Sauvegarde en format wide (une colonne par entreprise)\n",
    "df_env.to_csv('../data/clean/env_score_clean.csv', index=False)\n",
    "df_soc.to_csv('../data/clean/soc_score_clean.csv', index=False)\n",
    "df_esg.to_csv('../data/clean/esg_score_clean.csv', index=False)\n",
    "\n",
    "# Sauvegarde des variations annuelles\n",
    "df_env_var.to_csv('../data/clean/env_score_variations.csv', index=False)\n",
    "df_soc_var.to_csv('../data/clean/soc_score_variations.csv', index=False)\n",
    "df_esg_var.to_csv('../data/clean/esg_score_variations.csv', index=False)\n",
    "\n",
    "# Sauvegarde du panel complet\n",
    "df_panel.to_csv('../data/clean/panel_complet.csv', index=False)\n",
    "\n",
    "print(\"Fichiers sauvegardes dans data/clean/ :\")\n",
    "for f in sorted(os.listdir('../data/clean')):\n",
    "    size = os.path.getsize(f'../data/clean/{f}')\n",
    "    print(f\"  {f:40s} ({size/1024:.1f} Ko)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Synthese du Notebook 1\n",
    "\n",
    "### Donnees\n",
    "- **Panel** : 23 entreprises technologiques/innovantes, 122 observations mensuelles (2016-2026)\n",
    "- **Scores** : BESG Environmental Pillar, BESG Social Pillar, BESG ESG Score (composite)\n",
    "- **Source** : Bloomberg Terminal\n",
    "\n",
    "### Observations principales\n",
    "- Les valeurs manquantes sont concentrees en debut de serie (left-censoring) : plusieurs entreprises n'avaient pas de score ESG avant 2017-2020\n",
    "- Les scores sont sur une echelle 0-10, avec une tendance generale a la hausse sur la periode\n",
    "- La dispersion entre entreprises est significative, ce qui justifie l'application de methodes de clustering\n",
    "- Les correlations entre piliers Environmental et Social sont a examiner pour justifier ou non l'utilisation des deux dimensions separement\n",
    "\n",
    "### Prochaines etapes\n",
    "- **Notebook 2** : Application du K-means et du clustering hierarchique agglomeratif (CAH)\n",
    "- **Notebook 3** : Ameliorations methodologiques (ACP, DBSCAN, GMM, analyse temporelle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
