{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3 -- Ameliorations methodologiques et analyses avancees\n",
    "\n",
    "## Objectif\n",
    "\n",
    "Ce notebook adresse les limites identifiees dans le paper de reference et dans le Notebook 2, \n",
    "en implementant des ameliorations methodologiques :\n",
    "\n",
    "1. **Analyse en Composantes Principales (ACP)** avant clustering : reduction de dimension \n",
    "   et decorrelation des variables\n",
    "2. **Methodes de clustering alternatives** :\n",
    "   - DBSCAN (density-based) : detection de clusters de forme arbitraire et d'outliers\n",
    "   - Gaussian Mixture Models (GMM) : clustering probabiliste avec clusters ellipso\\u00efdaux\n",
    "3. **Analyse temporelle des clusters** : comment les affectations evoluent-elles dans le temps ?\n",
    "4. **Distances alternatives** : Manhattan, correlation, cosinus\n",
    "5. **Validation croisee du clustering** (bootstrap)\n",
    "6. **Interpretation financiere** et synthese\n",
    "\n",
    "## Pre-requis\n",
    "\n",
    "Executer les Notebooks 1 et 2 au prealable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Imports et chargement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.cm import get_cmap\n",
    "import seaborn as sns\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score, silhouette_samples,\n",
    "    calinski_harabasz_score, davies_bouldin_score,\n",
    "    adjusted_rand_score, normalized_mutual_info_score\n",
    ")\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (14, 7)\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['grid.alpha'] = 0.3\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "TICKER_NAMES = {\n",
    "    'NVDA US Equity': 'NVIDIA', 'AVGO US Equity': 'Broadcom',\n",
    "    'TSM US Equity': 'TSMC', 'GOOGL US Equity': 'Alphabet',\n",
    "    '000660 KS Equity': 'SK Hynix', 'LRCX US Equity': 'Lam Research',\n",
    "    '6857 JP Equity': 'Advantest', 'TSEM IT Equity': 'Tower Semi',\n",
    "    'MSFT US Equity': 'Microsoft', 'LITE US Equity': 'Lumentum',\n",
    "    'AMD US Equity': 'AMD', 'FN US Equity': 'Fabrinet',\n",
    "    'SNOW US Equity': 'Snowflake', 'MU US Equity': 'Micron',\n",
    "    'TSLA US Equity': 'Tesla', '9984 JP Equity': 'SoftBank',\n",
    "    'CRDO US Equity': 'CREDO Tech', 'ENR GR Equity': 'Siemens Energy',\n",
    "    'MPWR US Equity': 'Monolithic Power', '2383 TT Equity': 'Elite Material',\n",
    "    'CLS CN Equity': 'Celestica', 'META US Equity': 'Meta',\n",
    "    'AMZN US Equity': 'Amazon'\n",
    "}\n",
    "\n",
    "print('Imports effectues.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des donnees\n",
    "df_env = pd.read_csv('../data/clean/env_score_clean.csv', parse_dates=['Date'])\n",
    "df_soc = pd.read_csv('../data/clean/soc_score_clean.csv', parse_dates=['Date'])\n",
    "df_esg = pd.read_csv('../data/clean/esg_score_clean.csv', parse_dates=['Date'])\n",
    "\n",
    "mat_niveaux = pd.read_csv('../data/clean/clustering_niveaux.csv', index_col=0)\n",
    "mat_var = pd.read_csv('../data/clean/clustering_variations.csv', index_col=0)\n",
    "X_scaled_df = pd.read_csv('../data/clean/scores_standardises.csv', index_col=0)\n",
    "\n",
    "features = ['Env_Score', 'Soc_Score', 'ESG_Score']\n",
    "X_scaled = X_scaled_df.values\n",
    "\n",
    "print(f\"Donnees chargees : {X_scaled.shape[0]} entreprises, {X_scaled.shape[1]} variables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Analyse en Composantes Principales (ACP)\n",
    "\n",
    "### Justification\n",
    "\n",
    "L'ACP est une technique de reduction de dimension qui transforme les variables originales \n",
    "(potentiellement correlees) en composantes principales orthogonales. Dans notre contexte :\n",
    "\n",
    "- Les scores Environnemental, Social et ESG global sont construits de maniere a ce que le score \n",
    "  ESG soit une combinaison ponderee des piliers. Il existe donc une correlation structurelle.\n",
    "- L'ACP permet d'identifier les axes de variance principaux et de reduire le bruit.\n",
    "- Le clustering sur les composantes principales peut etre plus robuste que sur les variables originales.\n",
    "\n",
    "### Choix du nombre de composantes\n",
    "\n",
    "On utilise le critere de Kaiser (valeurs propres > 1 sur donnees standardisees) \n",
    "et le critere du coude sur le scree plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACP complete (3 composantes)\n",
    "pca_full = PCA(n_components=3)\n",
    "X_pca_full = pca_full.fit_transform(X_scaled)\n",
    "\n",
    "# Resultats\n",
    "print(\"=== Analyse en Composantes Principales ===\")\n",
    "print(f\"\\nValeurs propres (variance expliquee) :\")\n",
    "for i, (var, ratio) in enumerate(zip(pca_full.explained_variance_, \n",
    "                                      pca_full.explained_variance_ratio_)):\n",
    "    print(f\"  PC{i+1} : valeur propre = {var:.3f}, variance expliquee = {ratio*100:.1f}%\")\n",
    "\n",
    "print(f\"\\nVariance cumulee :\")\n",
    "cumvar = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "for i, cv in enumerate(cumvar):\n",
    "    print(f\"  PC1-PC{i+1} : {cv*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scree plot et variance cumulee\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Scree plot\n",
    "axes[0].bar(range(1, 4), pca_full.explained_variance_ratio_ * 100, \n",
    "            color='steelblue', edgecolor='black', alpha=0.8)\n",
    "axes[0].plot(range(1, 4), pca_full.explained_variance_ratio_ * 100, 'ro-', markersize=8)\n",
    "axes[0].axhline(y=100/3, color='red', linestyle='--', alpha=0.5, \n",
    "                label='Seuil equipartition (33.3%)')\n",
    "axes[0].set_xlabel('Composante principale')\n",
    "axes[0].set_ylabel('Variance expliquee (%)')\n",
    "axes[0].set_title('Scree Plot')\n",
    "axes[0].set_xticks([1, 2, 3])\n",
    "axes[0].legend()\n",
    "\n",
    "# Variance cumulee\n",
    "axes[1].plot(range(1, 4), cumvar * 100, 'bo-', linewidth=2, markersize=10)\n",
    "axes[1].axhline(y=80, color='red', linestyle='--', alpha=0.5, label='Seuil 80%')\n",
    "axes[1].axhline(y=90, color='orange', linestyle='--', alpha=0.5, label='Seuil 90%')\n",
    "axes[1].set_xlabel('Nombre de composantes')\n",
    "axes[1].set_ylabel('Variance cumulee (%)')\n",
    "axes[1].set_title('Variance expliquee cumulee')\n",
    "axes[1].set_xticks([1, 2, 3])\n",
    "axes[1].set_ylim([0, 105])\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../notebooks/figures/03_pca_scree.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cercle des correlations (loadings)\n",
    "loadings = pca_full.components_.T  # (n_features, n_components)\n",
    "loading_df = pd.DataFrame(loadings, columns=['PC1', 'PC2', 'PC3'], index=features)\n",
    "\n",
    "print(\"Matrice des loadings (contributions des variables aux composantes) :\")\n",
    "print(loading_df.round(3).to_string())\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "# Cercle unite\n",
    "theta = np.linspace(0, 2 * np.pi, 100)\n",
    "ax.plot(np.cos(theta), np.sin(theta), 'k--', linewidth=0.5, alpha=0.3)\n",
    "\n",
    "# Fleches des variables\n",
    "colors = ['#2ca02c', '#1f77b4', '#d62728']\n",
    "labels_var = ['Environmental', 'Social', 'ESG Global']\n",
    "for i, (feat, label, color) in enumerate(zip(features, labels_var, colors)):\n",
    "    ax.annotate('', xy=(loadings[i, 0], loadings[i, 1]), xytext=(0, 0),\n",
    "                arrowprops=dict(arrowstyle='->', color=color, lw=2.5))\n",
    "    offset = 0.05\n",
    "    ax.text(loadings[i, 0] + offset, loadings[i, 1] + offset, label,\n",
    "            fontsize=12, color=color, fontweight='bold')\n",
    "\n",
    "ax.set_xlim([-1.2, 1.2])\n",
    "ax.set_ylim([-1.2, 1.2])\n",
    "ax.set_xlabel(f'PC1 ({pca_full.explained_variance_ratio_[0]*100:.1f}%)')\n",
    "ax.set_ylabel(f'PC2 ({pca_full.explained_variance_ratio_[1]*100:.1f}%)')\n",
    "ax.set_title('Cercle des correlations (ACP)')\n",
    "ax.axhline(y=0, color='gray', linewidth=0.5)\n",
    "ax.axvline(x=0, color='gray', linewidth=0.5)\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../notebooks/figures/03_pca_cercle_correlations.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Projection des individus sur le plan PC1-PC2\n",
    "fig, ax = plt.subplots(figsize=(12, 9))\n",
    "\n",
    "ax.scatter(X_pca_full[:, 0], X_pca_full[:, 1], s=100, c='steelblue', \n",
    "           edgecolors='black', alpha=0.8)\n",
    "\n",
    "for i, ticker in enumerate(X_scaled_df.index):\n",
    "    name = TICKER_NAMES.get(ticker, ticker)\n",
    "    ax.annotate(name, (X_pca_full[i, 0], X_pca_full[i, 1]),\n",
    "                textcoords='offset points', xytext=(6, 6), fontsize=9)\n",
    "\n",
    "ax.axhline(y=0, color='gray', linewidth=0.5)\n",
    "ax.axvline(x=0, color='gray', linewidth=0.5)\n",
    "ax.set_xlabel(f'PC1 ({pca_full.explained_variance_ratio_[0]*100:.1f}% de variance)')\n",
    "ax.set_ylabel(f'PC2 ({pca_full.explained_variance_ratio_[1]*100:.1f}% de variance)')\n",
    "ax.set_title('Carte factorielle ACP -- Projection des entreprises')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../notebooks/figures/03_pca_individus.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering sur les composantes principales\n",
    "\n",
    "On applique K-means et CAH sur les composantes principales retenues (PC1, PC2) \n",
    "puis on compare les resultats avec ceux obtenus sur les variables originales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determination du nombre de composantes a retenir\n",
    "# Critere : on retient les composantes dont la variance expliquee cumulee depasse 80%\n",
    "n_components = np.argmax(cumvar >= 0.80) + 1\n",
    "print(f\"Nombre de composantes retenues (>=80% variance) : {n_components}\")\n",
    "\n",
    "# Si on n'a que 3 variables, 2 composantes suffisent generalement\n",
    "n_components = min(n_components, 2)\n",
    "X_pca = X_pca_full[:, :n_components]\n",
    "print(f\"Composantes utilisees pour le clustering : PC1 a PC{n_components}\")\n",
    "print(f\"Variance totale retenue : {cumvar[n_components-1]*100:.1f}%\")\n",
    "\n",
    "# K-means sur ACP\n",
    "K_range = range(2, min(8, len(X_pca)))\n",
    "sil_pca = []\n",
    "for k in K_range:\n",
    "    km_pca = KMeans(n_clusters=k, n_init=50, random_state=42)\n",
    "    labels_pca = km_pca.fit_predict(X_pca)\n",
    "    sil_pca.append(silhouette_score(X_pca, labels_pca))\n",
    "\n",
    "k_opt_pca = list(K_range)[np.argmax(sil_pca)]\n",
    "print(f\"\\nK optimal sur ACP : k = {k_opt_pca} (silhouette = {max(sil_pca):.3f})\")\n",
    "\n",
    "# Application\n",
    "km_pca_final = KMeans(n_clusters=k_opt_pca, n_init=50, random_state=42)\n",
    "labels_pca = km_pca_final.fit_predict(X_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison clustering original vs clustering sur ACP\n",
    "labels_original = mat_niveaux['Cluster_KM'].values\n",
    "ari_pca_vs_original = adjusted_rand_score(labels_original, labels_pca)\n",
    "\n",
    "print(f\"Concordance clustering original vs clustering ACP :\")\n",
    "print(f\"  ARI = {ari_pca_vs_original:.3f}\")\n",
    "print(f\"  Interpretation : {'forte concordance' if ari_pca_vs_original > 0.7 else 'concordance moderee' if ari_pca_vs_original > 0.4 else 'faible concordance'}\")\n",
    "\n",
    "# Visualisation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "cmap = get_cmap('Set2')\n",
    "\n",
    "for ax, (labels, titre) in zip(axes, [\n",
    "    (labels_original, 'K-means sur variables originales'),\n",
    "    (labels_pca + 1, 'K-means sur composantes ACP')\n",
    "]):\n",
    "    for c in sorted(np.unique(labels)):\n",
    "        mask = labels == c\n",
    "        ax.scatter(X_pca_full[mask, 0], X_pca_full[mask, 1],\n",
    "                   c=[cmap((c-1)/max(np.unique(labels)))], label=f'Cluster {c}', s=120,\n",
    "                   edgecolors='black', linewidth=1, alpha=0.8)\n",
    "    \n",
    "    for i, name in enumerate(mat_niveaux['Entreprise'].values):\n",
    "        ax.annotate(name, (X_pca_full[i, 0], X_pca_full[i, 1]),\n",
    "                    textcoords='offset points', xytext=(5, 5), fontsize=7)\n",
    "    \n",
    "    ax.set_xlabel(f'PC1 ({pca_full.explained_variance_ratio_[0]*100:.1f}%)')\n",
    "    ax.set_ylabel(f'PC2 ({pca_full.explained_variance_ratio_[1]*100:.1f}%)')\n",
    "    ax.set_title(titre)\n",
    "    ax.legend()\n",
    "\n",
    "plt.suptitle('Impact de l\\'ACP sur le clustering', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../notebooks/figures/03_pca_clustering_comparaison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. DBSCAN -- Clustering par densite\n",
    "\n",
    "### Motivation\n",
    "\n",
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) ne suppose pas \n",
    "de forme particuliere pour les clusters et detecte automatiquement les outliers (points \n",
    "de bruit). C'est une amelioration par rapport au K-means qui impose des clusters spheriques.\n",
    "\n",
    "### Parametres\n",
    "- `eps` : rayon du voisinage\n",
    "- `min_samples` : nombre minimum de points dans le voisinage pour former un cluster\n",
    "\n",
    "On optimise ces parametres par recherche en grille."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse du k-distance graph pour choisir eps\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Pour min_samples = nombre de dimensions + 1 (regle empirique)\n",
    "min_samples_default = X_scaled.shape[1] + 1  # 4\n",
    "\n",
    "nn = NearestNeighbors(n_neighbors=min_samples_default)\n",
    "nn.fit(X_scaled)\n",
    "distances, _ = nn.kneighbors(X_scaled)\n",
    "k_distances = np.sort(distances[:, -1])  # Distance au k-ieme voisin\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(range(len(k_distances)), k_distances, 'b-', linewidth=2)\n",
    "ax.set_xlabel('Points (tries par distance croissante)')\n",
    "ax.set_ylabel(f'Distance au {min_samples_default}-ieme voisin')\n",
    "ax.set_title(f'k-Distance Graph (min_samples = {min_samples_default})')\n",
    "\n",
    "# Identification du coude (approximatif)\n",
    "# On cherche le point d'inflexion maximale\n",
    "diffs = np.diff(k_distances)\n",
    "elbow_idx = np.argmax(diffs) + 1\n",
    "eps_suggestion = k_distances[elbow_idx]\n",
    "ax.axhline(y=eps_suggestion, color='red', linestyle='--', \n",
    "           label=f'eps suggere = {eps_suggestion:.2f}')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../notebooks/figures/03_dbscan_kdistance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"eps suggere par le k-distance graph : {eps_suggestion:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recherche en grille pour DBSCAN\n",
    "eps_values = np.arange(0.5, 3.5, 0.25)\n",
    "min_samples_values = [2, 3, 4, 5]\n",
    "\n",
    "dbscan_results = []\n",
    "\n",
    "for eps in eps_values:\n",
    "    for ms in min_samples_values:\n",
    "        db = DBSCAN(eps=eps, min_samples=ms)\n",
    "        labels_db = db.fit_predict(X_scaled)\n",
    "        n_clusters = len(set(labels_db)) - (1 if -1 in labels_db else 0)\n",
    "        n_noise = (labels_db == -1).sum()\n",
    "        \n",
    "        sil = np.nan\n",
    "        if n_clusters >= 2 and n_noise < len(labels_db) - 1:\n",
    "            # Silhouette sur les points non-bruit uniquement\n",
    "            mask_non_noise = labels_db != -1\n",
    "            if len(np.unique(labels_db[mask_non_noise])) >= 2:\n",
    "                sil = silhouette_score(X_scaled[mask_non_noise], labels_db[mask_non_noise])\n",
    "        \n",
    "        dbscan_results.append({\n",
    "            'eps': eps, 'min_samples': ms,\n",
    "            'n_clusters': n_clusters, 'n_noise': n_noise,\n",
    "            'silhouette': sil\n",
    "        })\n",
    "\n",
    "dbscan_df = pd.DataFrame(dbscan_results)\n",
    "# Filtrer les configurations avec au moins 2 clusters\n",
    "valid = dbscan_df[(dbscan_df['n_clusters'] >= 2) & (dbscan_df['silhouette'].notna())]\n",
    "\n",
    "if len(valid) > 0:\n",
    "    best_dbscan = valid.loc[valid['silhouette'].idxmax()]\n",
    "    print(f\"Meilleure configuration DBSCAN :\")\n",
    "    print(f\"  eps = {best_dbscan['eps']}, min_samples = {int(best_dbscan['min_samples'])}\")\n",
    "    print(f\"  Clusters = {int(best_dbscan['n_clusters'])}, Bruit = {int(best_dbscan['n_noise'])}\")\n",
    "    print(f\"  Silhouette = {best_dbscan['silhouette']:.3f}\")\n",
    "else:\n",
    "    print(\"Aucune configuration DBSCAN n'a produit au moins 2 clusters.\")\n",
    "    print(\"Cela peut indiquer que les donnees n'ont pas de structure de densite claire.\")\n",
    "    print(\"\\nConfigurations testees :\")\n",
    "    print(dbscan_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application de DBSCAN avec les meilleurs parametres\n",
    "if len(valid) > 0:\n",
    "    db_final = DBSCAN(eps=best_dbscan['eps'], min_samples=int(best_dbscan['min_samples']))\n",
    "    labels_dbscan = db_final.fit_predict(X_scaled)\n",
    "    \n",
    "    # Visualisation\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    unique_labels = sorted(set(labels_dbscan))\n",
    "    cmap = get_cmap('Set2')\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        mask = labels_dbscan == label\n",
    "        if label == -1:\n",
    "            ax.scatter(X_pca_full[mask, 0], X_pca_full[mask, 1], \n",
    "                       c='black', marker='x', s=100, label='Bruit (outliers)', zorder=5)\n",
    "        else:\n",
    "            ax.scatter(X_pca_full[mask, 0], X_pca_full[mask, 1],\n",
    "                       c=[cmap(label / max(1, max(unique_labels)))], \n",
    "                       label=f'Cluster {label+1}', s=120,\n",
    "                       edgecolors='black', linewidth=1, alpha=0.8)\n",
    "    \n",
    "    for i, ticker in enumerate(X_scaled_df.index):\n",
    "        name = TICKER_NAMES.get(ticker, ticker)\n",
    "        ax.annotate(name, (X_pca_full[i, 0], X_pca_full[i, 1]),\n",
    "                    textcoords='offset points', xytext=(5, 5), fontsize=8)\n",
    "    \n",
    "    ax.set_xlabel(f'PC1 ({pca_full.explained_variance_ratio_[0]*100:.1f}%)')\n",
    "    ax.set_ylabel(f'PC2 ({pca_full.explained_variance_ratio_[1]*100:.1f}%)')\n",
    "    ax.set_title(f'DBSCAN (eps={best_dbscan[\"eps\"]:.2f}, min_samples={int(best_dbscan[\"min_samples\"])})')\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../notebooks/figures/03_dbscan_result.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Entreprises identifiees comme outliers\n",
    "    outliers = X_scaled_df.index[labels_dbscan == -1]\n",
    "    if len(outliers) > 0:\n",
    "        print(f\"\\nEntreprises identifiees comme outliers par DBSCAN :\")\n",
    "        for t in outliers:\n",
    "            print(f\"  - {TICKER_NAMES.get(t, t)}\")\n",
    "    else:\n",
    "        print(\"\\nAucun outlier detecte par DBSCAN.\")\n",
    "else:\n",
    "    print(\"DBSCAN n'a pas pu etre applique avec des resultats significatifs.\")\n",
    "    print(\"Cela constitue en soi un resultat : la structure des donnees ne presente\")\n",
    "    print(\"pas de clusters a densite variable evidents, ce qui favorise les approches\")\n",
    "    print(\"partitionnelles (K-means) ou hierarchiques (CAH).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Gaussian Mixture Models (GMM)\n",
    "\n",
    "### Motivation\n",
    "\n",
    "Les GMM modelisent les donnees comme un melange de distributions gaussiennes. \n",
    "Contrairement au K-means :\n",
    "- Les clusters peuvent etre ellipsoidaux (pas necessairement spheriques)\n",
    "- Chaque point a une probabilite d'appartenance a chaque cluster (soft clustering)\n",
    "- On peut utiliser le BIC (Bayesian Information Criterion) pour choisir le nombre de composantes\n",
    "\n",
    "Le type de covariance determine la forme des clusters :\n",
    "- `full` : clusters ellipsoidaux quelconques\n",
    "- `tied` : meme forme pour tous les clusters\n",
    "- `diag` : axes alignes sur les axes de coordonnees\n",
    "- `spherical` : clusters spheriques (equivalent au K-means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selection du modele par BIC et AIC\n",
    "K_range_gmm = range(2, min(8, len(X_scaled)))\n",
    "cov_types = ['full', 'tied', 'diag', 'spherical']\n",
    "\n",
    "gmm_results = []\n",
    "\n",
    "for cov_type in cov_types:\n",
    "    for k in K_range_gmm:\n",
    "        gmm = GaussianMixture(n_components=k, covariance_type=cov_type, \n",
    "                               n_init=10, random_state=42, max_iter=500)\n",
    "        gmm.fit(X_scaled)\n",
    "        labels_gmm = gmm.predict(X_scaled)\n",
    "        \n",
    "        sil = silhouette_score(X_scaled, labels_gmm) if len(np.unique(labels_gmm)) >= 2 else np.nan\n",
    "        \n",
    "        gmm_results.append({\n",
    "            'k': k, 'covariance': cov_type,\n",
    "            'BIC': gmm.bic(X_scaled), 'AIC': gmm.aic(X_scaled),\n",
    "            'silhouette': sil,\n",
    "            'log_likelihood': gmm.score(X_scaled)\n",
    "        })\n",
    "\n",
    "gmm_df = pd.DataFrame(gmm_results)\n",
    "\n",
    "# Meilleur modele par BIC\n",
    "best_gmm = gmm_df.loc[gmm_df['BIC'].idxmin()]\n",
    "print(f\"Meilleur modele GMM (BIC minimal) :\")\n",
    "print(f\"  k = {int(best_gmm['k'])}, covariance = {best_gmm['covariance']}\")\n",
    "print(f\"  BIC = {best_gmm['BIC']:.1f}, AIC = {best_gmm['AIC']:.1f}\")\n",
    "print(f\"  Silhouette = {best_gmm['silhouette']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation BIC pour chaque type de covariance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "colors_cov = {'full': '#d62728', 'tied': '#2ca02c', 'diag': '#1f77b4', 'spherical': '#ff7f0e'}\n",
    "\n",
    "for cov_type in cov_types:\n",
    "    subset = gmm_df[gmm_df['covariance'] == cov_type]\n",
    "    axes[0].plot(subset['k'], subset['BIC'], 'o-', color=colors_cov[cov_type], \n",
    "                 label=cov_type, linewidth=2, markersize=8)\n",
    "    axes[1].plot(subset['k'], subset['AIC'], 'o-', color=colors_cov[cov_type],\n",
    "                 label=cov_type, linewidth=2, markersize=8)\n",
    "\n",
    "axes[0].set_xlabel('Nombre de composantes (k)')\n",
    "axes[0].set_ylabel('BIC')\n",
    "axes[0].set_title('BIC par type de covariance (a minimiser)')\n",
    "axes[0].legend()\n",
    "axes[0].set_xticks(list(K_range_gmm))\n",
    "\n",
    "axes[1].set_xlabel('Nombre de composantes (k)')\n",
    "axes[1].set_ylabel('AIC')\n",
    "axes[1].set_title('AIC par type de covariance (a minimiser)')\n",
    "axes[1].legend()\n",
    "axes[1].set_xticks(list(K_range_gmm))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../notebooks/figures/03_gmm_bic_aic.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application du meilleur GMM\n",
    "gmm_final = GaussianMixture(\n",
    "    n_components=int(best_gmm['k']), \n",
    "    covariance_type=best_gmm['covariance'],\n",
    "    n_init=10, random_state=42, max_iter=500\n",
    ")\n",
    "gmm_final.fit(X_scaled)\n",
    "labels_gmm = gmm_final.predict(X_scaled)\n",
    "proba_gmm = gmm_final.predict_proba(X_scaled)\n",
    "\n",
    "# Probabilites d'appartenance\n",
    "proba_df = pd.DataFrame(proba_gmm, \n",
    "                         columns=[f'P(Cluster {i+1})' for i in range(int(best_gmm['k']))],\n",
    "                         index=X_scaled_df.index)\n",
    "proba_df['Cluster'] = labels_gmm + 1\n",
    "proba_df['Entreprise'] = proba_df.index.map(TICKER_NAMES)\n",
    "proba_df['Certitude'] = proba_gmm.max(axis=1)\n",
    "\n",
    "print(f\"=== GMM ({best_gmm['covariance']}, k={int(best_gmm['k'])}) ===\\n\")\n",
    "print(\"Probabilites d'appartenance et certitude :\")\n",
    "print(proba_df.sort_values('Certitude').round(3).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation GMM avec ellipses de confiance\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "pca_2d = PCA(n_components=2)\n",
    "X_pca_2d = pca_2d.fit_transform(X_scaled)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 9))\n",
    "cmap = get_cmap('Set2')\n",
    "k_gmm = int(best_gmm['k'])\n",
    "\n",
    "# Points\n",
    "for c in range(k_gmm):\n",
    "    mask = labels_gmm == c\n",
    "    ax.scatter(X_pca_2d[mask, 0], X_pca_2d[mask, 1],\n",
    "               c=[cmap(c / k_gmm)], label=f'Cluster {c+1}', s=120,\n",
    "               edgecolors='black', linewidth=1, alpha=0.8)\n",
    "\n",
    "# Annotations\n",
    "for i, ticker in enumerate(X_scaled_df.index):\n",
    "    name = TICKER_NAMES.get(ticker, ticker)\n",
    "    ax.annotate(name, (X_pca_2d[i, 0], X_pca_2d[i, 1]),\n",
    "                textcoords='offset points', xytext=(5, 5), fontsize=8)\n",
    "\n",
    "# Ellipses de confiance (projetees en 2D)\n",
    "means_pca = pca_2d.transform(gmm_final.means_)\n",
    "for c in range(k_gmm):\n",
    "    ax.plot(means_pca[c, 0], means_pca[c, 1], 'kX', markersize=15, \n",
    "            markeredgewidth=2, zorder=5)\n",
    "\n",
    "ax.set_xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]*100:.1f}%)')\n",
    "ax.set_ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]*100:.1f}%)')\n",
    "ax.set_title(f'Gaussian Mixture Model ({best_gmm[\"covariance\"]}, k={k_gmm})')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../notebooks/figures/03_gmm_result.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Impact des metriques de distance\n",
    "\n",
    "Le paper original utilise exclusivement la distance euclidienne. \n",
    "On teste ici l'impact d'autres metriques sur le clustering hierarchique :\n",
    "\n",
    "- **Euclidienne** : sensible a l'amplitude, standard pour Ward\n",
    "- **Manhattan** (L1) : plus robuste aux outliers\n",
    "- **Cosinus** : mesure la similarite directionnelle (independante de l'amplitude)\n",
    "- **Correlation** : similaire au cosinus mais centree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAH avec differentes distances\n",
    "distance_metrics = ['euclidean', 'cityblock', 'cosine', 'correlation']\n",
    "distance_names = ['Euclidienne', 'Manhattan', 'Cosinus', 'Correlation']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 14))\n",
    "short_labels = [TICKER_NAMES.get(t, t)[:12] for t in X_scaled_df.index]\n",
    "\n",
    "cah_distance_results = {}\n",
    "\n",
    "for ax, metric, name in zip(axes.flat, distance_metrics, distance_names):\n",
    "    # Ward ne fonctionne qu'avec la distance euclidienne\n",
    "    # Pour les autres distances, on utilise le lien average\n",
    "    method = 'ward' if metric == 'euclidean' else 'average'\n",
    "    \n",
    "    Z = linkage(X_scaled, method=method, metric=metric)\n",
    "    \n",
    "    dendrogram(Z, labels=short_labels, leaf_rotation=90, leaf_font_size=8, ax=ax)\n",
    "    ax.set_title(f'Distance {name} (lien {method})')\n",
    "    ax.set_ylabel('Distance')\n",
    "    \n",
    "    # Sauvegarde pour comparaison\n",
    "    k = mat_niveaux['Cluster_KM'].nunique()\n",
    "    labels_dist = fcluster(Z, t=k, criterion='maxclust')\n",
    "    cah_distance_results[name] = labels_dist\n",
    "\n",
    "plt.suptitle('Impact de la metrique de distance sur la CAH', fontsize=14, y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../notebooks/figures/03_distances_dendrogrammes.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison des partitions obtenues avec differentes distances\n",
    "print(\"Concordance (ARI) entre partitions obtenues avec differentes distances :\\n\")\n",
    "\n",
    "distance_list = list(cah_distance_results.keys())\n",
    "ari_matrix = np.zeros((len(distance_list), len(distance_list)))\n",
    "\n",
    "for i, d1 in enumerate(distance_list):\n",
    "    for j, d2 in enumerate(distance_list):\n",
    "        ari_matrix[i, j] = adjusted_rand_score(cah_distance_results[d1], cah_distance_results[d2])\n",
    "\n",
    "ari_df = pd.DataFrame(ari_matrix, index=distance_list, columns=distance_list)\n",
    "print(ari_df.round(3).to_string())\n",
    "\n",
    "# Heatmap\n",
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "sns.heatmap(ari_df, annot=True, fmt='.3f', cmap='YlOrRd', vmin=0, vmax=1,\n",
    "            square=True, ax=ax)\n",
    "ax.set_title('ARI entre partitions CAH avec differentes distances')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../notebooks/figures/03_distances_ari.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Analyse temporelle des clusters\n",
    "\n",
    "### Motivation\n",
    "\n",
    "Les analyses precedentes traitent les scores ESG de maniere statique (moyenne sur la periode). \n",
    "Or, les scores evoluent dans le temps et les entreprises peuvent changer de cluster. \n",
    "L'analyse temporelle permet de :\n",
    "\n",
    "- Identifier les entreprises dont le profil ESG se transforme\n",
    "- Detecter des convergences ou divergences entre groupes\n",
    "- Evaluer la stabilite temporelle de la structure de clustering\n",
    "\n",
    "On applique K-means a chaque date (coupe transversale) et on suit l'evolution \n",
    "des affectations au cours du temps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering temporel : K-means a chaque date\n",
    "score_cols = [c for c in df_esg.columns if c != 'Date']\n",
    "k_temporal = mat_niveaux['Cluster_KM'].nunique()  # Meme k que l'analyse statique\n",
    "\n",
    "# On ne garde que les dates ou suffisamment d'entreprises ont un score\n",
    "min_obs = max(k_temporal + 1, 5)  # Au moins k+1 observations\n",
    "\n",
    "temporal_clusters = pd.DataFrame(index=df_esg['Date'])\n",
    "temporal_silhouettes = []\n",
    "\n",
    "for idx, row in df_esg.iterrows():\n",
    "    date = row['Date']\n",
    "    scores_env = df_env.iloc[idx][score_cols]\n",
    "    scores_soc = df_soc.iloc[idx][score_cols]\n",
    "    scores_esg = df_esg.iloc[idx][score_cols]\n",
    "    \n",
    "    # Matrice pour cette date\n",
    "    mat_t = pd.DataFrame({\n",
    "        'Env': scores_env.values,\n",
    "        'Soc': scores_soc.values,\n",
    "        'ESG': scores_esg.values\n",
    "    }, index=score_cols)\n",
    "    \n",
    "    # Supprimer les NA\n",
    "    mat_t = mat_t.dropna()\n",
    "    \n",
    "    if len(mat_t) >= min_obs:\n",
    "        scaler_t = StandardScaler()\n",
    "        X_t = scaler_t.fit_transform(mat_t)\n",
    "        \n",
    "        km_t = KMeans(n_clusters=k_temporal, n_init=20, random_state=42)\n",
    "        labels_t = km_t.fit_predict(X_t)\n",
    "        \n",
    "        sil_t = silhouette_score(X_t, labels_t) if len(np.unique(labels_t)) >= 2 else np.nan\n",
    "        temporal_silhouettes.append({'Date': date, 'Silhouette': sil_t, 'n_obs': len(mat_t)})\n",
    "        \n",
    "        for ticker, label in zip(mat_t.index, labels_t):\n",
    "            temporal_clusters.loc[date, ticker] = label + 1\n",
    "\n",
    "temporal_sil_df = pd.DataFrame(temporal_silhouettes)\n",
    "print(f\"Clustering temporel effectue sur {len(temporal_sil_df)} dates\")\n",
    "print(f\"Score de silhouette moyen : {temporal_sil_df['Silhouette'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evolution du score de silhouette dans le temps\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "axes[0].plot(temporal_sil_df['Date'], temporal_sil_df['Silhouette'], 'b-', linewidth=1.5)\n",
    "axes[0].axhline(y=temporal_sil_df['Silhouette'].mean(), color='red', linestyle='--',\n",
    "                label=f'Moyenne = {temporal_sil_df[\"Silhouette\"].mean():.3f}')\n",
    "axes[0].fill_between(temporal_sil_df['Date'], 0, temporal_sil_df['Silhouette'], alpha=0.2)\n",
    "axes[0].set_ylabel('Score de silhouette')\n",
    "axes[0].set_title(f'Evolution temporelle de la qualite du clustering (k={k_temporal})')\n",
    "axes[0].legend()\n",
    "axes[0].xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "\n",
    "axes[1].plot(temporal_sil_df['Date'], temporal_sil_df['n_obs'], 'g-', linewidth=1.5)\n",
    "axes[1].set_ylabel('Nombre d\\'entreprises avec score')\n",
    "axes[1].set_title('Nombre d\\'observations par date')\n",
    "axes[1].xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../notebooks/figures/03_temporal_silhouette.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap de l'evolution des clusters par entreprise\n",
    "# Selection d'un sous-ensemble d'entreprises pour la lisibilite\n",
    "fig, ax = plt.subplots(figsize=(18, 10))\n",
    "\n",
    "# Remplir les NaN pour la visualisation\n",
    "plot_data = temporal_clusters.dropna(axis=1, how='all').dropna(axis=0, how='all')\n",
    "\n",
    "# Renommer les colonnes\n",
    "plot_data.columns = [TICKER_NAMES.get(c, c)[:12] for c in plot_data.columns]\n",
    "\n",
    "# Creer un index lisible (annee-mois)\n",
    "plot_data.index = plot_data.index.strftime('%Y-%m')\n",
    "\n",
    "# Ne garder qu'un point tous les 3 mois pour la lisibilite\n",
    "plot_sampled = plot_data.iloc[::3]\n",
    "\n",
    "sns.heatmap(plot_sampled.T, cmap='Set2', cbar_kws={'label': 'Cluster'},\n",
    "            xticklabels=5, ax=ax, linewidths=0.1)\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Entreprise')\n",
    "ax.set_title('Evolution temporelle des affectations de cluster (K-means)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../notebooks/figures/03_temporal_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stabilite des affectations : combien de fois chaque entreprise change de cluster ?\n",
    "print(\"Nombre de changements de cluster par entreprise :\\n\")\n",
    "\n",
    "stability = {}\n",
    "for col in temporal_clusters.columns:\n",
    "    series = temporal_clusters[col].dropna()\n",
    "    if len(series) > 1:\n",
    "        n_changes = (series.diff() != 0).sum() - 1  # -1 pour la premiere difference (NaN)\n",
    "        n_obs = len(series)\n",
    "        stability[TICKER_NAMES.get(col, col)] = {\n",
    "            'n_changements': int(n_changes),\n",
    "            'n_observations': n_obs,\n",
    "            'taux_stabilite': 1 - n_changes / max(n_obs - 1, 1)\n",
    "        }\n",
    "\n",
    "stab_df = pd.DataFrame(stability).T.sort_values('taux_stabilite', ascending=True)\n",
    "print(stab_df.to_string())\n",
    "\n",
    "print(f\"\\nTaux de stabilite moyen : {stab_df['taux_stabilite'].mean():.3f}\")\n",
    "print(f\"Entreprise la plus stable : {stab_df.index[-1]} ({stab_df.iloc[-1]['taux_stabilite']:.3f})\")\n",
    "print(f\"Entreprise la moins stable : {stab_df.index[0]} ({stab_df.iloc[0]['taux_stabilite']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Validation par bootstrap\n",
    "\n",
    "Le bootstrap permet d'evaluer la robustesse du clustering en reevaluant \n",
    "les affectations sur des echantillons de donnees aleatoires (avec remplacement). \n",
    "C'est une amelioration methodologique qui renforce la confiance dans les resultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap du clustering\n",
    "n_bootstrap = 200\n",
    "k_boot = mat_niveaux['Cluster_KM'].nunique()\n",
    "n_samples = X_scaled.shape[0]\n",
    "\n",
    "# Matrice de co-occurrence : combien de fois deux entreprises sont dans le meme cluster ?\n",
    "cooccurrence = np.zeros((n_samples, n_samples))\n",
    "boot_silhouettes = []\n",
    "\n",
    "np.random.seed(42)\n",
    "for b in range(n_bootstrap):\n",
    "    # Echantillon bootstrap (avec remplacement)\n",
    "    boot_idx = np.random.choice(n_samples, size=n_samples, replace=True)\n",
    "    X_boot = X_scaled[boot_idx]\n",
    "    \n",
    "    km_boot = KMeans(n_clusters=k_boot, n_init=10, random_state=b)\n",
    "    labels_boot = km_boot.fit_predict(X_boot)\n",
    "    \n",
    "    # Predire les labels pour TOUS les points (pas juste le bootstrap)\n",
    "    labels_all = km_boot.predict(X_scaled)\n",
    "    \n",
    "    sil_boot = silhouette_score(X_scaled, labels_all)\n",
    "    boot_silhouettes.append(sil_boot)\n",
    "    \n",
    "    # Mise a jour de la matrice de co-occurrence\n",
    "    for i in range(n_samples):\n",
    "        for j in range(i + 1, n_samples):\n",
    "            if labels_all[i] == labels_all[j]:\n",
    "                cooccurrence[i, j] += 1\n",
    "                cooccurrence[j, i] += 1\n",
    "\n",
    "cooccurrence /= n_bootstrap  # Normaliser en frequences\n",
    "\n",
    "print(f\"Bootstrap termine ({n_bootstrap} iterations)\")\n",
    "print(f\"Silhouette bootstrap : {np.mean(boot_silhouettes):.3f} +/- {np.std(boot_silhouettes):.3f}\")\n",
    "print(f\"IC 95% : [{np.percentile(boot_silhouettes, 2.5):.3f}, {np.percentile(boot_silhouettes, 97.5):.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de co-occurrence\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "short_names = [TICKER_NAMES.get(t, t)[:12] for t in X_scaled_df.index]\n",
    "co_df = pd.DataFrame(cooccurrence, index=short_names, columns=short_names)\n",
    "\n",
    "# Reordonner par cluster K-means pour mettre en evidence la structure\n",
    "order = mat_niveaux.sort_values('Cluster_KM').index\n",
    "order_names = [TICKER_NAMES.get(t, t)[:12] for t in order]\n",
    "co_df = co_df.loc[order_names, order_names]\n",
    "\n",
    "sns.heatmap(co_df, annot=True, fmt='.2f', cmap='YlOrRd', vmin=0, vmax=1,\n",
    "            ax=ax, annot_kws={'size': 7})\n",
    "ax.set_title(f'Matrice de co-occurrence bootstrap (k={k_boot}, {n_bootstrap} iterations)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../notebooks/figures/03_bootstrap_cooccurrence.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation : une valeur proche de 1 signifie que les deux entreprises\")\n",
    "print(\"sont systematiquement affectees au meme cluster, quel que soit l'echantillon.\")\n",
    "print(\"Une valeur proche de 0.5 indique une affectation instable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution bootstrap du score de silhouette\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.hist(boot_silhouettes, bins=25, edgecolor='black', alpha=0.7, color='steelblue', density=True)\n",
    "\n",
    "# IC 95%\n",
    "ci_low = np.percentile(boot_silhouettes, 2.5)\n",
    "ci_high = np.percentile(boot_silhouettes, 97.5)\n",
    "ax.axvline(x=ci_low, color='red', linestyle='--', linewidth=2, label=f'IC 2.5% = {ci_low:.3f}')\n",
    "ax.axvline(x=ci_high, color='red', linestyle='--', linewidth=2, label=f'IC 97.5% = {ci_high:.3f}')\n",
    "ax.axvline(x=np.mean(boot_silhouettes), color='black', linestyle='-', linewidth=2,\n",
    "           label=f'Moyenne = {np.mean(boot_silhouettes):.3f}')\n",
    "\n",
    "ax.set_xlabel('Score de silhouette')\n",
    "ax.set_ylabel('Densite')\n",
    "ax.set_title(f'Distribution bootstrap du score de silhouette (n={n_bootstrap})')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../notebooks/figures/03_bootstrap_silhouette.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Tableau de synthese comparative\n",
    "\n",
    "On rassemble l'ensemble des resultats de clustering dans un tableau comparatif unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tableau comparatif de toutes les methodes\n",
    "resultats = []\n",
    "\n",
    "# K-means original\n",
    "labels_km_orig = mat_niveaux['Cluster_KM'].values\n",
    "resultats.append({\n",
    "    'Methode': 'K-means (original)',\n",
    "    'k': len(np.unique(labels_km_orig)),\n",
    "    'Silhouette': silhouette_score(X_scaled, labels_km_orig),\n",
    "    'Calinski-Harabasz': calinski_harabasz_score(X_scaled, labels_km_orig),\n",
    "    'Davies-Bouldin': davies_bouldin_score(X_scaled, labels_km_orig)\n",
    "})\n",
    "\n",
    "# CAH Ward\n",
    "labels_cah_orig = mat_niveaux['Cluster_CAH'].values\n",
    "resultats.append({\n",
    "    'Methode': 'CAH Ward',\n",
    "    'k': len(np.unique(labels_cah_orig)),\n",
    "    'Silhouette': silhouette_score(X_scaled, labels_cah_orig),\n",
    "    'Calinski-Harabasz': calinski_harabasz_score(X_scaled, labels_cah_orig),\n",
    "    'Davies-Bouldin': davies_bouldin_score(X_scaled, labels_cah_orig)\n",
    "})\n",
    "\n",
    "# K-means sur ACP\n",
    "resultats.append({\n",
    "    'Methode': 'K-means (ACP)',\n",
    "    'k': k_opt_pca,\n",
    "    'Silhouette': silhouette_score(X_pca, labels_pca),\n",
    "    'Calinski-Harabasz': calinski_harabasz_score(X_pca, labels_pca),\n",
    "    'Davies-Bouldin': davies_bouldin_score(X_pca, labels_pca)\n",
    "})\n",
    "\n",
    "# GMM\n",
    "resultats.append({\n",
    "    'Methode': f'GMM ({best_gmm[\"covariance\"]})',\n",
    "    'k': int(best_gmm['k']),\n",
    "    'Silhouette': silhouette_score(X_scaled, labels_gmm),\n",
    "    'Calinski-Harabasz': calinski_harabasz_score(X_scaled, labels_gmm),\n",
    "    'Davies-Bouldin': davies_bouldin_score(X_scaled, labels_gmm)\n",
    "})\n",
    "\n",
    "# DBSCAN (si applicable)\n",
    "if len(valid) > 0:\n",
    "    mask_non_noise = labels_dbscan != -1\n",
    "    if len(np.unique(labels_dbscan[mask_non_noise])) >= 2:\n",
    "        resultats.append({\n",
    "            'Methode': 'DBSCAN',\n",
    "            'k': int(best_dbscan['n_clusters']),\n",
    "            'Silhouette': silhouette_score(X_scaled[mask_non_noise], labels_dbscan[mask_non_noise]),\n",
    "            'Calinski-Harabasz': calinski_harabasz_score(X_scaled[mask_non_noise], labels_dbscan[mask_non_noise]),\n",
    "            'Davies-Bouldin': davies_bouldin_score(X_scaled[mask_non_noise], labels_dbscan[mask_non_noise])\n",
    "        })\n",
    "\n",
    "synthese = pd.DataFrame(resultats).set_index('Methode')\n",
    "print(\"=\" * 70)\n",
    "print(\"SYNTHESE COMPARATIVE DES METHODES DE CLUSTERING\")\n",
    "print(\"=\" * 70)\n",
    "print(synthese.round(3).to_string())\n",
    "\n",
    "print(\"\\nRappel des criteres :\")\n",
    "print(\"  - Silhouette : a maximiser (entre -1 et 1)\")\n",
    "print(\"  - Calinski-Harabasz : a maximiser\")\n",
    "print(\"  - Davies-Bouldin : a minimiser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de concordance (ARI) entre toutes les methodes\n",
    "all_labels = {\n",
    "    'K-means': labels_km_orig,\n",
    "    'CAH Ward': labels_cah_orig,\n",
    "    'K-means ACP': labels_pca + 1,\n",
    "    'GMM': labels_gmm + 1\n",
    "}\n",
    "\n",
    "method_names = list(all_labels.keys())\n",
    "ari_all = np.zeros((len(method_names), len(method_names)))\n",
    "\n",
    "for i, m1 in enumerate(method_names):\n",
    "    for j, m2 in enumerate(method_names):\n",
    "        ari_all[i, j] = adjusted_rand_score(all_labels[m1], all_labels[m2])\n",
    "\n",
    "ari_all_df = pd.DataFrame(ari_all, index=method_names, columns=method_names)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(ari_all_df, annot=True, fmt='.3f', cmap='YlOrRd', vmin=0, vmax=1,\n",
    "            square=True, ax=ax)\n",
    "ax.set_title('Concordance (ARI) entre toutes les methodes de clustering')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../notebooks/figures/03_concordance_globale.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nMatrice de concordance ARI :\")\n",
    "print(ari_all_df.round(3).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Interpretation financiere et discussion\n",
    "\n",
    "### Profils ESG identifies\n",
    "\n",
    "Les clusters obtenus revelent des profils ESG distincts parmi les entreprises technologiques \n",
    "de notre panel. L'interpretation doit tenir compte de plusieurs elements :\n",
    "\n",
    "- Les scores BESG de Bloomberg sont bases sur la divulgation d'informations et les pratiques \n",
    "  observees. Un score faible ne signifie pas necessairement de mauvaises pratiques, \n",
    "  mais peut refleter un manque de transparence.\n",
    "- Le secteur technologique est structurellement different du secteur energetique analyse \n",
    "  dans le paper de reference : l'impact environnemental direct est moindre, \n",
    "  mais les enjeux sociaux (vie privee, IA responsable) sont majeurs.\n",
    "- L'heterogeneite geographique du panel (USA, Japon, Coree, Taiwan, Israel, Europe, Canada) \n",
    "  influence les scores en raison des differences reglementaires.\n",
    "\n",
    "### Ameliorations implementees par rapport au paper\n",
    "\n",
    "| Limite du paper | Amelioration implementee | Resultat |\n",
    "|:---|:---|:---|\n",
    "| Distance euclidienne uniquement | Test de 4 metriques de distance | Concordance mesuree par ARI |\n",
    "| Pas de reduction de dimension | ACP avant clustering | Variance expliquee et impact |\n",
    "| K-means et CAH uniquement | DBSCAN et GMM en complement | Comparaison systematique |\n",
    "| Analyse statique | Clustering temporel (rolling) | Stabilite des affectations |\n",
    "| Pas de validation | Bootstrap (200 iterations) | IC 95% et matrice de co-occurrence |\n",
    "| Choix de k subjectif | 4 criteres (silhouette, CH, DB, BIC) | Criteres concordants ou non |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tableau final : affectation de chaque entreprise par toutes les methodes\n",
    "tableau_final = pd.DataFrame(index=mat_niveaux.index)\n",
    "tableau_final['Entreprise'] = tableau_final.index.map(TICKER_NAMES)\n",
    "tableau_final['Env_Score'] = mat_niveaux['Env_Score'].round(2)\n",
    "tableau_final['Soc_Score'] = mat_niveaux['Soc_Score'].round(2)\n",
    "tableau_final['ESG_Score'] = mat_niveaux['ESG_Score'].round(2)\n",
    "tableau_final['K-means'] = mat_niveaux['Cluster_KM']\n",
    "tableau_final['CAH'] = mat_niveaux['Cluster_CAH']\n",
    "tableau_final['K-means ACP'] = labels_pca + 1\n",
    "tableau_final['GMM'] = labels_gmm + 1\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TABLEAU FINAL : AFFECTATION DES ENTREPRISES PAR METHODE\")\n",
    "print(\"=\" * 80)\n",
    "print(tableau_final.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde du tableau final\n",
    "import os\n",
    "os.makedirs('../data/clean', exist_ok=True)\n",
    "os.makedirs('../notebooks/figures', exist_ok=True)\n",
    "\n",
    "tableau_final.to_csv('../data/clean/resultats_finaux.csv')\n",
    "synthese.to_csv('../data/clean/synthese_metriques.csv')\n",
    "\n",
    "print(\"Resultats finaux sauvegardes :\")\n",
    "print(\"  - data/clean/resultats_finaux.csv\")\n",
    "print(\"  - data/clean/synthese_metriques.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Synthese du Notebook 3\n",
    "\n",
    "### Ameliorations implementees\n",
    "\n",
    "1. **ACP** : decorrelation et reduction de dimension avant clustering. Le cercle des correlations \n",
    "   et la carte factorielle apportent une comprehension plus fine de la structure des donnees.\n",
    "\n",
    "2. **DBSCAN** : detection de clusters de forme arbitraire et d'outliers ESG. Permet d'identifier \n",
    "   les entreprises au profil atypique.\n",
    "\n",
    "3. **GMM** : clustering probabiliste, chaque entreprise a une probabilite d'appartenance \n",
    "   a chaque cluster. Le BIC permet une selection rigoureuse du modele.\n",
    "\n",
    "4. **Metriques de distance** : la concordance entre partitions obtenues avec differentes distances \n",
    "   permet de juger de la robustesse de la structure.\n",
    "\n",
    "5. **Analyse temporelle** : mise en evidence des trajectoires ESG et de la stabilite des clusters.\n",
    "\n",
    "6. **Bootstrap** : quantification de l'incertitude via les intervalles de confiance \n",
    "   et la matrice de co-occurrence.\n",
    "\n",
    "### Limites residuelles\n",
    "\n",
    "- L'absence de rendements boursiers dans nos donnees empeche l'analyse du lien ESG/performance financiere\n",
    "- Le panel de 23 entreprises reste de taille moderee pour des methodes comme DBSCAN\n",
    "- Les scores BESG sont mis a jour irregulierement, ce qui peut introduire un biais dans l'analyse temporelle\n",
    "- La composante Gouvernance n'est pas presente dans nos donnees (seuls les piliers E et S sont disponibles)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
